{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Baseline Modeling with Logistic Regression\n",
    "\n",
    "## Objectives\n",
    "1. Load the processed dataset from Week 1\n",
    "2. Split data into train/test sets with stratification\n",
    "3. Scale features appropriately\n",
    "4. Train a dummy baseline classifier\n",
    "5. Train logistic regression with class balancing\n",
    "6. Evaluate models comprehensively\n",
    "7. Interpret model coefficients\n",
    "8. Save model and results\n",
    "\n",
    "## Timeline\n",
    "**Week 2 (Nov 25 - Dec 1)** - Due: Dec 1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_curve, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Random seed\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "project_root = Path.cwd().parent\n",
    "data_dir = project_root / 'data'\n",
    "processed_data_dir = data_dir / 'processed'\n",
    "models_dir = project_root / 'models'\n",
    "figures_dir = project_root / 'figures'\n",
    "\n",
    "# Create directories\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Models directory: {models_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset created in Week 1\n",
    "data_file = processed_data_dir / 'hits_dataset.csv'\n",
    "\n",
    "if not data_file.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"❌ Dataset not found at {data_file}\\n\"\n",
    "        \"Please run notebook 01_Week1_Data_Setup_EDA.ipynb first!\"\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(data_file)\n",
    "print(f\"✅ Loaded dataset: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['is_hit'].value_counts())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature columns (numeric columns except target and metadata)\n",
    "# Exclude: is_hit, year, track names, artist names\n",
    "exclude_cols = ['is_hit', 'year']\n",
    "\n",
    "# Also exclude any text columns\n",
    "text_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "exclude_cols.extend(text_cols)\n",
    "\n",
    "# Get feature columns\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"Feature columns ({len(feature_cols)}):\")\n",
    "print(feature_cols)\n",
    "\n",
    "# Prepare X and y\n",
    "X = df[feature_cols].values\n",
    "y = df['is_hit'].values\n",
    "\n",
    "print(f\"\\nX shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  Class {label}: {count:,} ({count/len(y)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/Test Split\n",
    "\n",
    "**Important:** We use stratified split to maintain class balance in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified train/test split\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=y  # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {len(X_train):,} ({(1-TEST_SIZE)*100:.0f}%)\")\n",
    "print(f\"Test set size:  {len(X_test):,} ({TEST_SIZE*100:.0f}%)\")\n",
    "\n",
    "print(f\"\\nTrain set class distribution:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  Class {label}: {count:,} ({count/len(y_train)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  Class {label}: {count:,} ({count/len(y_test)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Scaling\n",
    "\n",
    "Logistic regression benefits from scaled features (mean=0, std=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit scaler on training data only (avoid data leakage)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"✅ Features scaled\")\n",
    "print(f\"\\nTraining set statistics (after scaling):\")\n",
    "print(f\"  Mean: {X_train_scaled.mean():.6f} (should be ~0)\")\n",
    "print(f\"  Std:  {X_train_scaled.std():.6f} (should be ~1)\")\n",
    "\n",
    "# Save scaler for future use\n",
    "joblib.dump(scaler, models_dir / 'scaler.pkl')\n",
    "print(f\"\\n✅ Saved scaler to {models_dir / 'scaler.pkl'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline Model: Dummy Classifier\n",
    "\n",
    "Before building a real model, let's establish a baseline using a simple strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy classifier (predicts most frequent class)\n",
    "dummy_clf = DummyClassifier(strategy='most_frequent', random_state=RANDOM_SEED)\n",
    "dummy_clf.fit(X_train_scaled, y_train)\n",
    "y_pred_dummy = dummy_clf.predict(X_test_scaled)\n",
    "\n",
    "print(\"DUMMY CLASSIFIER RESULTS (Baseline)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Strategy: Always predict most frequent class\")\n",
    "print(f\"\\nAccuracy:  {accuracy_score(y_test, y_pred_dummy):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_dummy, zero_division=0):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred_dummy, zero_division=0):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_dummy, zero_division=0):.4f}\")\n",
    "\n",
    "print(f\"\\n⚠️  This is our baseline to beat!\")\n",
    "print(f\"    A good model should perform significantly better than this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Logistic Regression with Class Balancing\n",
    "\n",
    "**Key Strategy:** Use `class_weight='balanced'` to handle class imbalance.\n",
    "\n",
    "This automatically adjusts weights to be inversely proportional to class frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression\n",
    "print(\"Training Logistic Regression...\\n\")\n",
    "\n",
    "logreg = LogisticRegression(\n",
    "    class_weight='balanced',  # Handle class imbalance\n",
    "    random_state=RANDOM_SEED,\n",
    "    max_iter=1000,\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"✅ Model trained successfully\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = logreg.predict(X_test_scaled)\n",
    "y_pred_proba = logreg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"✅ Predictions generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation\n",
    "\n",
    "### 7.1 Basic Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "pr_auc = average_precision_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"LOGISTIC REGRESSION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {accuracy:.4f}  ← Overall correctness\")\n",
    "print(f\"Precision: {precision:.4f}  ← Of predicted hits, how many are correct?\")\n",
    "print(f\"Recall:    {recall:.4f}  ← Of actual hits, how many did we catch?\")\n",
    "print(f\"F1 Score:  {f1:.4f}  ← Harmonic mean of precision and recall\")\n",
    "print(f\"ROC-AUC:   {roc_auc:.4f}  ← Overall discrimination ability\")\n",
    "print(f\"PR-AUC:    {pr_auc:.4f}  ← Precision-recall trade-off (better for imbalanced data)\")\n",
    "\n",
    "print(f\"\\n⚠️  For imbalanced data, focus on:\")\n",
    "print(f\"   • Recall (catch as many hits as possible)\")\n",
    "print(f\"   • F1 Score (balance precision and recall)\")\n",
    "print(f\"   • PR-AUC (more informative than ROC-AUC for imbalanced data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Non-Hit', 'Hit'],\n",
    "            yticklabels=['Non-Hit', 'Hit'],\n",
    "            cbar_kws={'label': 'Count'},\n",
    "            ax=ax)\n",
    "\n",
    "ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax.set_ylabel('True Label', fontsize=12)\n",
    "ax.set_title('Confusion Matrix - Logistic Regression', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add percentage annotations\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        percentage = cm[i, j] / cm[i].sum() * 100\n",
    "        ax.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)', \n",
    "                ha='center', va='center', fontsize=10, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'logreg_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Saved: logreg_confusion_matrix.png\")\n",
    "\n",
    "# Print detailed breakdown\n",
    "print(f\"\\nConfusion Matrix Breakdown:\")\n",
    "print(f\"  True Negatives  (TN): {cm[0, 0]:,}  ← Correctly predicted non-hits\")\n",
    "print(f\"  False Positives (FP): {cm[0, 1]:,}  ← Non-hits wrongly predicted as hits\")\n",
    "print(f\"  False Negatives (FN): {cm[1, 0]:,}  ← Hits wrongly predicted as non-hits\")\n",
    "print(f\"  True Positives  (TP): {cm[1, 1]:,}  ← Correctly predicted hits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred, \n",
    "                          target_names=['Non-Hit', 'Hit'],\n",
    "                          digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.plot(fpr, tpr, linewidth=2, label=f'Logistic Regression (AUC = {roc_auc:.4f})')\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier (AUC = 0.5000)')\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curve - Logistic Regression', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'logreg_roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Saved: logreg_roc_curve.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Precision-Recall Curve\n",
    "\n",
    "**More informative than ROC for imbalanced datasets!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall curve\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.plot(recall_curve, precision_curve, linewidth=2, \n",
    "        label=f'Logistic Regression (AP = {pr_auc:.4f})')\n",
    "\n",
    "# Baseline (random classifier for imbalanced data)\n",
    "baseline = (y_test == 1).sum() / len(y_test)\n",
    "ax.axhline(y=baseline, color='k', linestyle='--', linewidth=1,\n",
    "          label=f'Random Classifier (AP = {baseline:.4f})')\n",
    "\n",
    "ax.set_xlabel('Recall', fontsize=12)\n",
    "ax.set_ylabel('Precision', fontsize=12)\n",
    "ax.set_title('Precision-Recall Curve - Logistic Regression', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right', fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'logreg_pr_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Saved: logreg_pr_curve.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance (Coefficients)\n",
    "\n",
    "Logistic regression coefficients tell us which features are most important for predicting hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coefficients\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Coefficient': logreg.coef_[0]\n",
    "})\n",
    "coefficients['Abs_Coefficient'] = np.abs(coefficients['Coefficient'])\n",
    "coefficients = coefficients.sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"Feature Coefficients (sorted by absolute value):\")\n",
    "print(\"=\"*60)\n",
    "for idx, row in coefficients.iterrows():\n",
    "    direction = \"increases\" if row['Coefficient'] > 0 else \"decreases\"\n",
    "    print(f\"{row['Feature']:20s} {row['Coefficient']:+.4f}  ← {direction} hit probability\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  • Positive coefficient: Higher feature value → More likely to be a HIT\")\n",
    "print(f\"  • Negative coefficient: Higher feature value → More likely to be a NON-HIT\")\n",
    "print(f\"  • Larger absolute value: Stronger effect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coefficients\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors = ['#e74c3c' if c < 0 else '#2ecc71' for c in coefficients['Coefficient']]\n",
    "bars = ax.barh(range(len(coefficients)), coefficients['Coefficient'], color=colors)\n",
    "\n",
    "ax.set_yticks(range(len(coefficients)))\n",
    "ax.set_yticklabels(coefficients['Feature'])\n",
    "ax.set_xlabel('Coefficient Value', fontsize=12)\n",
    "ax.set_title('Feature Coefficients - Logistic Regression', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#2ecc71', label='Increases hit probability'),\n",
    "    Patch(facecolor='#e74c3c', label='Decreases hit probability')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'logreg_coefficients.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Saved: logreg_coefficients.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_file = models_dir / 'baseline_logreg.pkl'\n",
    "joblib.dump(logreg, model_file)\n",
    "\n",
    "print(f\"✅ Saved model to: {model_file}\")\n",
    "\n",
    "# Save metrics for comparison later\n",
    "metrics = {\n",
    "    'model': 'Logistic Regression',\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1_score': f1,\n",
    "    'roc_auc': roc_auc,\n",
    "    'pr_auc': pr_auc\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "metrics_df.to_csv(models_dir / 'baseline_metrics.csv', index=False)\n",
    "print(f\"✅ Saved metrics to: {models_dir / 'baseline_metrics.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"WEEK 2 SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. MODEL PERFORMANCE\")\n",
    "print(f\"\\n   Dummy Baseline (Most Frequent):\")\n",
    "print(f\"     Accuracy: {accuracy_score(y_test, y_pred_dummy):.4f}\")\n",
    "print(f\"     F1 Score: {f1_score(y_test, y_pred_dummy, zero_division=0):.4f}\")\n",
    "\n",
    "print(f\"\\n   Logistic Regression (Balanced):\")\n",
    "print(f\"     Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"     Precision: {precision:.4f}\")\n",
    "print(f\"     Recall:    {recall:.4f}\")\n",
    "print(f\"     F1 Score:  {f1:.4f}\")\n",
    "print(f\"     ROC-AUC:   {roc_auc:.4f}\")\n",
    "print(f\"     PR-AUC:    {pr_auc:.4f}\")\n",
    "\n",
    "print(f\"\\n2. KEY INSIGHTS\")\n",
    "print(f\"\\n   Top 3 Positive Predictors (increase hit probability):\")\n",
    "top_positive = coefficients[coefficients['Coefficient'] > 0].head(3)\n",
    "for i, (idx, row) in enumerate(top_positive.iterrows(), 1):\n",
    "    print(f\"     {i}. {row['Feature']}: {row['Coefficient']:+.4f}\")\n",
    "\n",
    "print(f\"\\n   Top 3 Negative Predictors (decrease hit probability):\")\n",
    "top_negative = coefficients[coefficients['Coefficient'] < 0].head(3)\n",
    "for i, (idx, row) in enumerate(top_negative.iterrows(), 1):\n",
    "    print(f\"     {i}. {row['Feature']}: {row['Coefficient']:+.4f}\")\n",
    "\n",
    "print(f\"\\n3. MODEL STRENGTHS & WEAKNESSES\")\n",
    "if recall > 0.6:\n",
    "    print(f\"   ✅ Good recall: Catching {recall*100:.1f}% of actual hits\")\n",
    "else:\n",
    "    print(f\"   ⚠️  Low recall: Only catching {recall*100:.1f}% of actual hits\")\n",
    "\n",
    "if precision > 0.5:\n",
    "    print(f\"   ✅ Decent precision: {precision*100:.1f}% of predicted hits are correct\")\n",
    "else:\n",
    "    print(f\"   ⚠️  Low precision: Only {precision*100:.1f}% of predicted hits are correct\")\n",
    "\n",
    "print(f\"\\n4. NEXT STEPS (Week 3)\")\n",
    "print(f\"   → Train XGBoost model (handle non-linear patterns)\")\n",
    "print(f\"   → Hyperparameter tuning\")\n",
    "print(f\"   → SHAP analysis for deeper interpretation\")\n",
    "print(f\"   → Compare XGBoost vs Logistic Regression\")\n",
    "\n",
    "print(f\"\\n5. FILES CREATED\")\n",
    "print(f\"   → {model_file}\")\n",
    "print(f\"   → {models_dir / 'scaler.pkl'}\")\n",
    "print(f\"   → {figures_dir / 'logreg_confusion_matrix.png'}\")\n",
    "print(f\"   → {figures_dir / 'logreg_roc_curve.png'}\")\n",
    "print(f\"   → {figures_dir / 'logreg_pr_curve.png'}\")\n",
    "print(f\"   → {figures_dir / 'logreg_coefficients.png'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ WEEK 2 COMPLETE - Baseline model established!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ✅ Week 2 Deliverables Complete!\n",
    "\n",
    "You now have:\n",
    "1. ✅ Trained logistic regression baseline\n",
    "2. ✅ Comprehensive evaluation metrics\n",
    "3. ✅ Feature importance analysis\n",
    "4. ✅ Multiple visualizations\n",
    "5. ✅ Saved model for future comparison\n",
    "\n",
    "### Key Takeaways:\n",
    "- Logistic regression provides an **interpretable** baseline\n",
    "- Class balancing helps handle imbalanced data\n",
    "- For imbalanced data, **F1, Recall, and PR-AUC** are more important than accuracy\n",
    "- Coefficients reveal which audio features matter most for hits\n",
    "\n",
    "### Next Steps:\n",
    "Proceed to **03_Week3_XGBoost_SHAP.ipynb** for advanced modeling with XGBoost and SHAP interpretation.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
