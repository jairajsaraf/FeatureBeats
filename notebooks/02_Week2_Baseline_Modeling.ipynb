{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 2 \u2013 Baseline Modeling (Logistic Regression)\n",
        "\n",
        "This notebook builds a robust baseline using a dummy classifier and logistic regression with class balancing. It reports accuracy, precision, recall, F1, ROC-AUC, PR-AUC, and saves evaluation plots and the trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (ConfusionMatrixDisplay, average_precision_score,\n",
        "                             classification_report, confusion_matrix, precision_recall_curve,\n",
        "                             roc_auc_score, roc_curve)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "DATA_PATH = Path('data/processed/hits_dataset.csv')\n",
        "FIG_DIR = Path('figures')\n",
        "MODEL_DIR = Path('models')\n",
        "FIG_DIR.mkdir(exist_ok=True)\n",
        "MODEL_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "assert DATA_PATH.exists(), 'Run Week 1 notebook first to generate hits_dataset.csv'\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "feature_candidates = [\n",
        "    'danceability', 'energy', 'loudness', 'speechiness', 'acousticness',\n",
        "    'instrumentalness', 'liveness', 'valence', 'tempo'\n",
        "]\n",
        "features = [f for f in feature_candidates if f in df.columns]\n",
        "target = 'is_hit'\n",
        "\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline: Dummy classifier\n",
        "\n",
        "A stratified dummy classifier establishes the performance floor for an imbalanced dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dummy = DummyClassifier(strategy='stratified', random_state=42)\n",
        "dummy.fit(X_train, y_train)\n",
        "dummy_probs = dummy.predict_proba(X_test)[:, 1]\n",
        "dummy_preds = dummy.predict(X_test)\n",
        "\n",
        "dummy_metrics = {\n",
        "    'accuracy': dummy.score(X_test, y_test),\n",
        "    'roc_auc': roc_auc_score(y_test, dummy_probs),\n",
        "    'pr_auc': average_precision_score(y_test, dummy_probs)\n",
        "}\n",
        "print('Dummy metrics:', dummy_metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logistic regression with class balancing\n",
        "\n",
        "We standardize features and enable `class_weight=\"balanced\"` to address severe imbalance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "log_reg = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', LogisticRegression(\n",
        "        max_iter=200,\n",
        "        class_weight='balanced',\n",
        "        solver='lbfgs'\n",
        "    ))\n",
        "])\n",
        "\n",
        "log_reg.fit(X_train, y_train)\n",
        "probs = log_reg.predict_proba(X_test)[:, 1]\n",
        "preds = log_reg.predict(X_test)\n",
        "\n",
        "metrics = {\n",
        "    'accuracy': log_reg.score(X_test, y_test),\n",
        "    'precision': np.mean(preds[y_test.values == 1]) if (preds == 1).sum() else 0.0,\n",
        "    'recall': np.mean(y_test.values[preds == 1]) if (preds == 1).sum() else 0.0,\n",
        "    'f1': None,\n",
        "    'roc_auc': roc_auc_score(y_test, probs),\n",
        "    'pr_auc': average_precision_score(y_test, probs)\n",
        "}\n",
        "precision_vals, recall_vals, _ = precision_recall_curve(y_test, probs)\n",
        "f1_scores = 2 * (precision_vals * recall_vals) / (precision_vals + recall_vals + 1e-9)\n",
        "metrics['f1'] = float(np.max(f1_scores))\n",
        "\n",
        "print('Logistic regression metrics:', metrics)\n",
        "print('\n",
        "Classification report:\n",
        "', classification_report(y_test, preds, digits=3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visual evaluation\n",
        "\n",
        "Confusion matrix, ROC curve, and precision-recall curve provide a holistic view of performance on the minority class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cm = confusion_matrix(y_test, preds, labels=[0, 1])\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=['Non-hit', 'Hit'])\n",
        "disp.plot(values_format='d')\n",
        "plt.title('Logistic Regression Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / 'logreg_confusion_matrix.png', dpi=300)\n",
        "plt.close()\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_test, probs)\n",
        "fig, ax = plt.subplots(figsize=(5, 4))\n",
        "ax.plot(fpr, tpr, label=f\"ROC AUC = {metrics['roc_auc']:.3f}\")\n",
        "ax.plot([0, 1], [0, 1], '--', color='gray')\n",
        "ax.set_title('ROC Curve')\n",
        "ax.set_xlabel('False Positive Rate')\n",
        "ax.set_ylabel('True Positive Rate')\n",
        "ax.legend()\n",
        "fig.tight_layout()\n",
        "fig.savefig(FIG_DIR / 'logreg_roc.png', dpi=300)\n",
        "plt.close(fig)\n",
        "\n",
        "precision_vals, recall_vals, _ = precision_recall_curve(y_test, probs)\n",
        "fig, ax = plt.subplots(figsize=(5, 4))\n",
        "ax.plot(recall_vals, precision_vals, label=f\"PR AUC = {metrics['pr_auc']:.3f}\")\n",
        "ax.set_title('Precision-Recall Curve')\n",
        "ax.set_xlabel('Recall')\n",
        "ax.set_ylabel('Precision')\n",
        "ax.legend()\n",
        "fig.tight_layout()\n",
        "fig.savefig(FIG_DIR / 'logreg_pr.png', dpi=300)\n",
        "plt.close(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Coefficient analysis and model saving\n",
        "\n",
        "Examine feature coefficients for interpretability and persist the trained baseline for comparison against XGBoost."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "coef = log_reg.named_steps['model'].coef_[0]\n",
        "coef_df = pd.DataFrame({'feature': features, 'coefficient': coef}).sort_values(by='coefficient', ascending=False)\n",
        "print(coef_df)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "sns.barplot(data=coef_df, x='coefficient', y='feature', ax=ax, palette='vlag')\n",
        "ax.set_title('Logistic Regression Coefficients')\n",
        "fig.tight_layout()\n",
        "fig.savefig(FIG_DIR / 'logreg_coefficients.png', dpi=300)\n",
        "plt.close(fig)\n",
        "\n",
        "MODEL_DIR.mkdir(exist_ok=True)\n",
        "model_path = MODEL_DIR / 'baseline_logreg.pkl'\n",
        "joblib.dump(log_reg, model_path)\n",
        "print(f'Saved baseline model to {model_path}')\n",
        "\n",
        "with open(MODEL_DIR / 'baseline_metrics.json', 'w') as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "print('Stored baseline metrics for downstream comparison.')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}