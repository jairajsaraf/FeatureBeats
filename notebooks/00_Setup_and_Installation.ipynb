{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 - Project Setup and Installation\n",
    "\n",
    "This notebook will help you:\n",
    "1. Verify your Python environment\n",
    "2. Install required packages\n",
    "3. Create necessary directories\n",
    "4. Test that everything is working correctly\n",
    "\n",
    "**Run this notebook first before proceeding to the analysis notebooks!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify Python Version\n",
    "\n",
    "We need Python 3.8 or higher for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "# Check version\n",
    "if sys.version_info >= (3, 8):\n",
    "    print(\"‚úÖ Python version is compatible!\")\n",
    "else:\n",
    "    print(\"‚ùå Please upgrade to Python 3.8 or higher\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Required Packages\n",
    "\n",
    "**Note:** It's recommended to run this in a virtual environment.\n",
    "\n",
    "### Option 1: Install from requirements.txt (recommended)\n",
    "\n",
    "Open a terminal in the project root directory and run:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Option 2: Install directly from this notebook\n",
    "\n",
    "Uncomment and run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Verify Package Installation\n",
    "\n",
    "Let's check that all required packages are installed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "\n",
    "required_packages = [\n",
    "    'numpy',\n",
    "    'pandas',\n",
    "    'sklearn',\n",
    "    'xgboost',\n",
    "    'shap',\n",
    "    'matplotlib',\n",
    "    'seaborn',\n",
    "    'imblearn',\n",
    "    'fuzzywuzzy',\n",
    "    'joblib',\n",
    "    'scipy',\n",
    "    'tqdm'\n",
    "]\n",
    "\n",
    "print(\"Checking installed packages...\\n\")\n",
    "all_installed = True\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        module = importlib.import_module(package)\n",
    "        version = getattr(module, '__version__', 'unknown')\n",
    "        print(f\"‚úÖ {package:20s} {version}\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ùå {package:20s} NOT INSTALLED\")\n",
    "        all_installed = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "if all_installed:\n",
    "    print(\"‚úÖ All required packages are installed!\")\n",
    "else:\n",
    "    print(\"‚ùå Some packages are missing. Please install them using requirements.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Verify Directory Structure\n",
    "\n",
    "Let's check that all necessary directories exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get project root (parent of notebooks directory)\n",
    "project_root = Path.cwd().parent\n",
    "print(f\"Project root: {project_root}\\n\")\n",
    "\n",
    "# Required directories\n",
    "required_dirs = [\n",
    "    'data/raw',\n",
    "    'data/processed',\n",
    "    'models',\n",
    "    'figures',\n",
    "    'reports',\n",
    "    'notebooks',\n",
    "    'src'\n",
    "]\n",
    "\n",
    "print(\"Checking directory structure...\\n\")\n",
    "all_exist = True\n",
    "\n",
    "for dir_path in required_dirs:\n",
    "    full_path = project_root / dir_path\n",
    "    if full_path.exists():\n",
    "        print(f\"‚úÖ {dir_path}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {dir_path} - MISSING\")\n",
    "        all_exist = False\n",
    "        # Create it\n",
    "        full_path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"   ‚Üí Created directory: {dir_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "if all_exist:\n",
    "    print(\"‚úÖ All directories exist!\")\n",
    "else:\n",
    "    print(\"‚úÖ Missing directories have been created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test Basic Functionality\n",
    "\n",
    "Let's run a quick test to make sure everything works together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Testing basic functionality...\\n\")\n",
    "\n",
    "# Create dummy data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(100, 5)\n",
    "y = np.random.randint(0, 2, 100)\n",
    "\n",
    "# Test train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"‚úÖ Train/test split works - Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
    "\n",
    "# Test model training\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"‚úÖ Model training works - Test accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Test plotting\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot([1, 2, 3], [1, 4, 9], marker='o')\n",
    "plt.title('Test Plot')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.close()  # Don't display, just test\n",
    "print(\"‚úÖ Plotting works\")\n",
    "\n",
    "# Test DataFrame creation\n",
    "df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(5)])\n",
    "df['target'] = y\n",
    "print(f\"‚úÖ Pandas works - DataFrame shape: {df.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚úÖ All basic functionality tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Set up Plotting Style\n",
    "\n",
    "Configure matplotlib and seaborn for nice visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "# Test the style\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "x = np.linspace(0, 10, 100)\n",
    "ax.plot(x, np.sin(x), label='sin(x)', linewidth=2)\n",
    "ax.plot(x, np.cos(x), label='cos(x)', linewidth=2)\n",
    "ax.set_xlabel('X axis')\n",
    "ax.set_ylabel('Y axis')\n",
    "ax.set_title('Test Plot with Configured Style')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Plotting style configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Data Download Instructions\n",
    "\n",
    "### Required Datasets\n",
    "\n",
    "You need to download two datasets from Kaggle and place them in the `data/raw/` directory:\n",
    "\n",
    "1. **Spotify Tracks Dataset**\n",
    "   - Search for \"Spotify Tracks Dataset\" on Kaggle\n",
    "   - Download `tracks.csv`\n",
    "   - Place in: `data/raw/tracks.csv`\n",
    "   - Should contain audio features for thousands of songs\n",
    "\n",
    "2. **Spotify Top 100 Dataset** (or Billboard Hot 100)\n",
    "   - Search for \"Spotify Top 100\" or \"Billboard Hot 100\" on Kaggle\n",
    "   - Download the CSV file\n",
    "   - Place in: `data/raw/top100_tracks.csv`\n",
    "   - Should contain chart-topping songs\n",
    "\n",
    "### Alternative Dataset Sources\n",
    "\n",
    "If you can't find these exact datasets, you can use:\n",
    "- **Spotify Million Song Dataset**\n",
    "- **Billboard Charts Dataset**\n",
    "- Any dataset with Spotify audio features and a way to identify hits\n",
    "\n",
    "**Just make sure the datasets have:**\n",
    "- Track name\n",
    "- Artist name\n",
    "- Audio features (danceability, energy, valence, etc.)\n",
    "- Year/date information\n",
    "- (Optional) Spotify track ID for better matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Verify Data Files (Optional)\n",
    "\n",
    "Run this cell after you've downloaded the datasets to verify they're in the right place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "data_dir = project_root / 'data' / 'raw'\n",
    "\n",
    "print(\"Checking for data files...\\n\")\n",
    "\n",
    "# Check for tracks dataset\n",
    "tracks_file = data_dir / 'tracks.csv'\n",
    "if tracks_file.exists():\n",
    "    size_mb = tracks_file.stat().st_size / (1024 * 1024)\n",
    "    print(f\"‚úÖ tracks.csv found ({size_mb:.2f} MB)\")\n",
    "    \n",
    "    # Try to read first few rows\n",
    "    try:\n",
    "        df_preview = pd.read_csv(tracks_file, nrows=5)\n",
    "        print(f\"   Columns: {', '.join(df_preview.columns[:10])}...\")\n",
    "        print(f\"   Shape preview: ({df_preview.shape[0]} rows shown, ? total)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Could not read file: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå tracks.csv not found\")\n",
    "    print(f\"   Expected location: {tracks_file}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Check for top 100 dataset\n",
    "top100_file = data_dir / 'top100_tracks.csv'\n",
    "if top100_file.exists():\n",
    "    size_mb = top100_file.stat().st_size / (1024 * 1024)\n",
    "    print(f\"‚úÖ top100_tracks.csv found ({size_mb:.2f} MB)\")\n",
    "    \n",
    "    # Try to read first few rows\n",
    "    try:\n",
    "        df_preview = pd.read_csv(top100_file, nrows=5)\n",
    "        print(f\"   Columns: {', '.join(df_preview.columns)}\")\n",
    "        print(f\"   Shape preview: ({df_preview.shape[0]} rows shown, ? total)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Could not read file: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå top100_tracks.csv not found\")\n",
    "    print(f\"   Expected location: {top100_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"If files are missing, please download them as per the instructions above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Setup Complete!\n",
    "\n",
    "If all checks passed, you're ready to proceed to the analysis notebooks:\n",
    "\n",
    "1. **01_Week1_Data_Setup_EDA.ipynb** - Data loading and exploration\n",
    "2. **02_Week2_Baseline_Modeling.ipynb** - Logistic regression baseline\n",
    "3. **03_Week3_XGBoost_SHAP.ipynb** - Advanced modeling and interpretation\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "If you encountered any issues:\n",
    "\n",
    "1. **Package installation failures:**\n",
    "   - Try updating pip: `pip install --upgrade pip`\n",
    "   - Install packages one by one to identify the problem\n",
    "   - Check Python version compatibility\n",
    "\n",
    "2. **Import errors:**\n",
    "   - Restart the Jupyter kernel\n",
    "   - Check that you're using the correct virtual environment\n",
    "   - Verify package installation with `pip list`\n",
    "\n",
    "3. **Data file issues:**\n",
    "   - Ensure files are in CSV format\n",
    "   - Check file permissions\n",
    "   - Verify file paths are correct\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Download the required datasets (see Step 7)\n",
    "2. Run the verification in Step 8\n",
    "3. Proceed to notebook 01 for data analysis\n",
    "\n",
    "Happy coding! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
