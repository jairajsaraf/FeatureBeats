{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: XGBoost Modeling and SHAP Interpretation\n",
    "\n",
    "## Objectives\n",
    "1. Load preprocessed data and baseline model\n",
    "2. Train XGBoost classifier with class imbalance handling\n",
    "3. Perform hyperparameter tuning\n",
    "4. Evaluate and compare with baseline\n",
    "5. Use SHAP for model interpretation\n",
    "6. Generate comprehensive visualizations\n",
    "7. Prepare final presentation materials\n",
    "\n",
    "## Timeline\n",
    "**Week 3 (Dec 2-8)** - Final Presentation: Dec 2\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import joblib\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_curve, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Random seed\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# Set this to True to skip hyperparameter tuning (faster, uses defaults)\n",
    "# Set to False for full hyperparameter search (slower, better results)\n",
    "SKIP_TUNING = False  # Change to True for quick testing\n",
    "\n",
    "if SKIP_TUNING:\n",
    "    print(\"âš ï¸  QUICK MODE: Skipping hyperparameter tuning\")\n",
    "    print(\"   Will use default parameters (faster but may not be optimal)\")\n",
    "else:\n",
    "    print(\"âœ… FULL MODE: Will perform hyperparameter tuning\")\n",
    "    print(\"   This may take 15-30 minutes depending on dataset size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "project_root = Path.cwd().parent\n",
    "data_dir = project_root / 'data'\n",
    "processed_data_dir = data_dir / 'processed'\n",
    "models_dir = project_root / 'models'\n",
    "figures_dir = project_root / 'figures'\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Models directory: {models_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_file = processed_data_dir / 'hits_dataset.csv'\n",
    "\n",
    "if not data_file.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"âŒ Dataset not found. Please run notebooks 01 and 02 first!\"\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(data_file)\n",
    "print(f\"âœ… Loaded dataset: {df.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['is_hit'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target (same as Week 2)\n",
    "exclude_cols = ['is_hit', 'year']\n",
    "text_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "exclude_cols.extend(text_cols)\n",
    "\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df['is_hit'].values\n",
    "\n",
    "print(f\"Features: {feature_cols}\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (same split as Week 2 for fair comparison)\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(X_train):,} samples\")\n",
    "print(f\"Test set:  {len(X_test):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline model for comparison\n",
    "baseline_file = models_dir / 'baseline_logreg.pkl'\n",
    "if baseline_file.exists():\n",
    "    baseline_model = joblib.load(baseline_file)\n",
    "    baseline_metrics = pd.read_csv(models_dir / 'baseline_metrics.csv')\n",
    "    print(\"âœ… Loaded baseline model\")\n",
    "    print(\"\\nBaseline performance:\")\n",
    "    print(baseline_metrics.to_string(index=False))\n",
    "else:\n",
    "    print(\"âš ï¸  Baseline model not found. Run notebook 02 first for comparison.\")\n",
    "    baseline_model = None\n",
    "    baseline_metrics = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. XGBoost with Default Parameters\n",
    "\n",
    "First, let's train XGBoost with reasonable default parameters to establish a quick baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate scale_pos_weight for class imbalance\n",
    "neg_count = (y_train == 0).sum()\n",
    "pos_count = (y_train == 1).sum()\n",
    "scale_pos_weight = neg_count / pos_count\n",
    "\n",
    "print(f\"Class imbalance handling:\")\n",
    "print(f\"  Negative samples: {neg_count:,}\")\n",
    "print(f\"  Positive samples: {pos_count:,}\")\n",
    "print(f\"  scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "print(f\"\\n  This parameter tells XGBoost to give {scale_pos_weight:.1f}x more weight to hits.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost with default parameters\n",
    "print(\"Training XGBoost with default parameters...\\n\")\n",
    "\n",
    "xgb_default = xgb.XGBClassifier(\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "xgb_default.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_default = xgb_default.predict(X_test)\n",
    "y_pred_proba_default = xgb_default.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"âœ… Default XGBoost trained\")\n",
    "print(f\"\\nQuick Performance Check:\")\n",
    "print(f\"  Accuracy:  {accuracy_score(y_test, y_pred_default):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_pred_default):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_test, y_pred_default):.4f}\")\n",
    "print(f\"  F1 Score:  {f1_score(y_test, y_pred_default):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Tuning (Optional)\n",
    "\n",
    "Use RandomizedSearchCV to find better hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_TUNING:\n",
    "    print(\"Starting hyperparameter tuning...\")\n",
    "    print(\"This may take 15-30 minutes.\\n\")\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_distributions = {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'max_depth': [3, 4, 5, 6, 7, 8],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'min_child_weight': [1, 3, 5, 7],\n",
    "        'gamma': [0, 0.1, 0.2, 0.3],\n",
    "    }\n",
    "    \n",
    "    # Base model\n",
    "    xgb_base = xgb.XGBClassifier(\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=RANDOM_SEED,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    \n",
    "    # Randomized search\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_base,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=20,  # Number of parameter combinations to try\n",
    "        cv=3,  # 3-fold cross-validation\n",
    "        scoring='f1',  # Optimize for F1 score (good for imbalanced data)\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1,  # Use all CPU cores\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"\\nâœ… Hyperparameter tuning complete!\")\n",
    "    print(f\"\\nBest parameters:\")\n",
    "    for param, value in random_search.best_params_.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    print(f\"\\nBest cross-validation F1 score: {random_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Use best model\n",
    "    xgb_final = random_search.best_estimator_\n",
    "    \n",
    "else:\n",
    "    print(\"â­ï¸  Skipping hyperparameter tuning (using default parameters)\")\n",
    "    xgb_final = xgb_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final Model Training with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with early stopping to prevent overfitting\n",
    "print(\"Training final XGBoost model with early stopping...\\n\")\n",
    "\n",
    "# Split training data into train/validation\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "# Train with early stopping\n",
    "xgb_final.fit(\n",
    "    X_train_split, y_train_split,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"âœ… Final model trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = xgb_final.predict(X_test)\n",
    "y_pred_proba = xgb_final.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "pr_auc = average_precision_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"XGBOOST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "print(f\"PR-AUC:    {pr_auc:.4f}\")\n",
    "\n",
    "# Compare with baseline\n",
    "if baseline_metrics is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARISON WITH BASELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    comparison = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC', 'PR-AUC'],\n",
    "        'Logistic Regression': [\n",
    "            baseline_metrics['accuracy'].values[0],\n",
    "            baseline_metrics['precision'].values[0],\n",
    "            baseline_metrics['recall'].values[0],\n",
    "            baseline_metrics['f1_score'].values[0],\n",
    "            baseline_metrics['roc_auc'].values[0],\n",
    "            baseline_metrics['pr_auc'].values[0]\n",
    "        ],\n",
    "        'XGBoost': [accuracy, precision, recall, f1, roc_auc, pr_auc]\n",
    "    })\n",
    "    \n",
    "    comparison['Improvement'] = comparison['XGBoost'] - comparison['Logistic Regression']\n",
    "    comparison['Improvement %'] = (comparison['Improvement'] / comparison['Logistic Regression'] * 100).round(1)\n",
    "    \n",
    "    print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=['Non-Hit', 'Hit'],\n",
    "            yticklabels=['Non-Hit', 'Hit'],\n",
    "            cbar_kws={'label': 'Count'},\n",
    "            ax=ax)\n",
    "\n",
    "ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax.set_ylabel('True Label', fontsize=12)\n",
    "ax.set_title('Confusion Matrix - XGBoost', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add percentage annotations\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        percentage = cm[i, j] / cm[i].sum() * 100\n",
    "        ax.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)', \n",
    "                ha='center', va='center', fontsize=10, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'xgboost_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Saved: xgboost_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred, \n",
    "                          target_names=['Non-Hit', 'Hit'],\n",
    "                          digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if baseline_metrics is not None:\n",
    "    # Side-by-side comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Metrics comparison bar chart\n",
    "    ax1 = axes[0]\n",
    "    x = np.arange(len(comparison['Metric']))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar(x - width/2, comparison['Logistic Regression'], width, \n",
    "                    label='Logistic Regression', color='#3498db')\n",
    "    bars2 = ax1.bar(x + width/2, comparison['XGBoost'], width,\n",
    "                    label='XGBoost', color='#2ecc71')\n",
    "    \n",
    "    ax1.set_xlabel('Metric', fontsize=12)\n",
    "    ax1.set_ylabel('Score', fontsize=12)\n",
    "    ax1.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(comparison['Metric'], rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    ax1.set_ylim([0, 1])\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Improvement visualization\n",
    "    ax2 = axes[1]\n",
    "    colors = ['#2ecc71' if x > 0 else '#e74c3c' for x in comparison['Improvement %']]\n",
    "    bars = ax2.barh(comparison['Metric'], comparison['Improvement %'], color=colors)\n",
    "    \n",
    "    ax2.set_xlabel('Improvement (%)', fontsize=12)\n",
    "    ax2.set_title('XGBoost Improvement over Baseline', fontsize=14, fontweight='bold')\n",
    "    ax2.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "    ax2.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        label_x = width + (1 if width > 0 else -1)\n",
    "        ax2.text(label_x, bar.get_y() + bar.get_height()/2.,\n",
    "                f'{width:+.1f}%', ha='left' if width > 0 else 'right', \n",
    "                va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figures_dir / 'model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Saved: model_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SHAP Analysis for Model Interpretation\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) helps us understand:\n",
    "- Which features are most important\n",
    "- How each feature affects predictions\n",
    "- Individual prediction explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating SHAP values...\")\n",
    "print(\"This may take a few minutes.\\n\")\n",
    "\n",
    "# Create SHAP explainer\n",
    "# Use a sample for faster computation if dataset is large\n",
    "sample_size = min(1000, len(X_test))\n",
    "X_test_sample = X_test[:sample_size]\n",
    "y_test_sample = y_test[:sample_size]\n",
    "\n",
    "explainer = shap.TreeExplainer(xgb_final)\n",
    "shap_values = explainer.shap_values(X_test_sample)\n",
    "\n",
    "print(f\"âœ… SHAP values calculated for {sample_size} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance based on mean absolute SHAP values\n",
    "shap_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': np.abs(shap_values).mean(axis=0)\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"SHAP Feature Importance:\")\n",
    "print(\"=\"*60)\n",
    "for idx, row in shap_importance.iterrows():\n",
    "    print(f\"{row['Feature']:20s} {row['Importance']:.4f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(shap_importance)))\n",
    "bars = ax.barh(range(len(shap_importance)), shap_importance['Importance'], color=colors)\n",
    "\n",
    "ax.set_yticks(range(len(shap_importance)))\n",
    "ax.set_yticklabels(shap_importance['Feature'])\n",
    "ax.set_xlabel('Mean |SHAP Value|', fontsize=12)\n",
    "ax.set_title('SHAP Feature Importance', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'shap_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Saved: shap_feature_importance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Summary Plot\n",
    "\n",
    "Shows both feature importance and effect direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot (beeswarm)\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_test_sample, \n",
    "                  feature_names=feature_cols,\n",
    "                  show=False)\n",
    "plt.title('SHAP Summary Plot - Feature Effects', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'shap_summary_detailed.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Saved: shap_summary_detailed.png\")\n",
    "print(\"\\nHow to read this plot:\")\n",
    "print(\"  â€¢ Each dot is a song\")\n",
    "print(\"  â€¢ Red = high feature value, Blue = low feature value\")\n",
    "print(\"  â€¢ Right side (positive SHAP) = increases hit probability\")\n",
    "print(\"  â€¢ Left side (negative SHAP) = decreases hit probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Dependence Plots\n",
    "\n",
    "Show how individual features affect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dependence plots for top 4 features\n",
    "top_features = shap_importance['Feature'].head(4).tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(top_features):\n",
    "    feature_idx = feature_cols.index(feature)\n",
    "    shap.dependence_plot(\n",
    "        feature_idx, \n",
    "        shap_values, \n",
    "        X_test_sample,\n",
    "        feature_names=feature_cols,\n",
    "        ax=axes[idx],\n",
    "        show=False\n",
    "    )\n",
    "    axes[idx].set_title(f'SHAP Dependence: {feature}', fontweight='bold')\n",
    "\n",
    "plt.suptitle('SHAP Dependence Plots - Top 4 Features', \n",
    "             fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'shap_dependence_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Saved: shap_dependence_plots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Force Plot (Sample Prediction Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain a single hit song prediction\n",
    "hit_indices = np.where(y_test_sample == 1)[0]\n",
    "if len(hit_indices) > 0:\n",
    "    sample_idx = hit_indices[0]  # First hit in test set\n",
    "    \n",
    "    print(f\"Explaining prediction for sample {sample_idx} (actual HIT)\")\n",
    "    print(f\"Predicted probability: {xgb_final.predict_proba(X_test_sample[sample_idx:sample_idx+1])[:, 1][0]:.4f}\")\n",
    "    \n",
    "    # Force plot\n",
    "    shap.force_plot(\n",
    "        explainer.expected_value,\n",
    "        shap_values[sample_idx],\n",
    "        X_test_sample[sample_idx],\n",
    "        feature_names=feature_cols,\n",
    "        matplotlib=True,\n",
    "        show=False\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figures_dir / 'shap_force_plot_example.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Saved: shap_force_plot_example.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Insights from SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"KEY INSIGHTS FROM SHAP ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. MOST IMPORTANT FEATURES FOR PREDICTING HITS:\")\n",
    "for i, (idx, row) in enumerate(shap_importance.head(5).iterrows(), 1):\n",
    "    print(f\"   {i}. {row['Feature']} (importance: {row['Importance']:.4f})\")\n",
    "\n",
    "print(f\"\\n2. FEATURE EFFECTS:\")\n",
    "for feature in top_features:\n",
    "    feature_idx = feature_cols.index(feature)\n",
    "    mean_shap = shap_values[:, feature_idx].mean()\n",
    "    \n",
    "    # Determine direction\n",
    "    if mean_shap > 0:\n",
    "        direction = \"INCREASES\"\n",
    "        emoji = \"ðŸ“ˆ\"\n",
    "    else:\n",
    "        direction = \"DECREASES\"\n",
    "        emoji = \"ðŸ“‰\"\n",
    "    \n",
    "    print(f\"   {emoji} Higher {feature} generally {direction} hit probability\")\n",
    "\n",
    "print(f\"\\n3. MODEL INTERPRETATION:\")\n",
    "print(f\"   â€¢ XGBoost captures non-linear relationships\")\n",
    "print(f\"   â€¢ SHAP values show individual feature contributions\")\n",
    "print(f\"   â€¢ Some features have complex interactions (see dependence plots)\")\n",
    "\n",
    "print(f\"\\n4. COMPARISON WITH LOGISTIC REGRESSION:\")\n",
    "if baseline_model is not None:\n",
    "    # Compare feature importance\n",
    "    lr_coef = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'LR_Coefficient': baseline_model.coef_[0]\n",
    "    })\n",
    "    \n",
    "    importance_comparison = shap_importance.merge(lr_coef, on='Feature')\n",
    "    importance_comparison['LR_Abs_Coef'] = np.abs(importance_comparison['LR_Coefficient'])\n",
    "    \n",
    "    print(f\"   Top features agree: \", end=\"\")\n",
    "    lr_top = set(lr_coef.nlargest(3, 'LR_Abs_Coef')['Feature'])\n",
    "    shap_top = set(shap_importance.head(3)['Feature'])\n",
    "    agreement = lr_top & shap_top\n",
    "    print(f\"{', '.join(agreement) if agreement else 'Different top features'}\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save XGBoost model\n",
    "model_file = models_dir / 'final_xgboost.pkl'\n",
    "joblib.dump(xgb_final, model_file)\n",
    "print(f\"âœ… Saved model to: {model_file}\")\n",
    "\n",
    "# Save metrics\n",
    "xgb_metrics = {\n",
    "    'model': 'XGBoost',\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1_score': f1,\n",
    "    'roc_auc': roc_auc,\n",
    "    'pr_auc': pr_auc\n",
    "}\n",
    "\n",
    "xgb_metrics_df = pd.DataFrame([xgb_metrics])\n",
    "xgb_metrics_df.to_csv(models_dir / 'xgboost_metrics.csv', index=False)\n",
    "print(f\"âœ… Saved metrics to: {models_dir / 'xgboost_metrics.csv'}\")\n",
    "\n",
    "# Save SHAP values for future use\n",
    "np.save(models_dir / 'shap_values.npy', shap_values)\n",
    "print(f\"âœ… Saved SHAP values to: {models_dir / 'shap_values.npy'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"WEEK 3 FINAL SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. FINAL MODEL PERFORMANCE\")\n",
    "print(f\"\\n   XGBoost Metrics:\")\n",
    "print(f\"     Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"     Precision: {precision:.4f}  â† {precision*100:.1f}% of predicted hits are correct\")\n",
    "print(f\"     Recall:    {recall:.4f}  â† Catching {recall*100:.1f}% of actual hits\")\n",
    "print(f\"     F1 Score:  {f1:.4f}  â† Balanced precision-recall\")\n",
    "print(f\"     ROC-AUC:   {roc_auc:.4f}\")\n",
    "print(f\"     PR-AUC:    {pr_auc:.4f}\")\n",
    "\n",
    "if baseline_metrics is not None:\n",
    "    print(f\"\\n2. IMPROVEMENT OVER BASELINE\")\n",
    "    f1_improvement = ((f1 - baseline_metrics['f1_score'].values[0]) / \n",
    "                     baseline_metrics['f1_score'].values[0] * 100)\n",
    "    recall_improvement = ((recall - baseline_metrics['recall'].values[0]) / \n",
    "                         baseline_metrics['recall'].values[0] * 100)\n",
    "    \n",
    "    print(f\"     F1 Score: {f1_improvement:+.1f}% improvement\")\n",
    "    print(f\"     Recall:   {recall_improvement:+.1f}% improvement\")\n",
    "    \n",
    "    if f1_improvement > 5:\n",
    "        print(f\"     âœ… Significant improvement! XGBoost is superior.\")\n",
    "    elif f1_improvement > 0:\n",
    "        print(f\"     âœ… Modest improvement. XGBoost performs slightly better.\")\n",
    "    else:\n",
    "        print(f\"     âš ï¸  Baseline was comparable. Consider feature engineering.\")\n",
    "\n",
    "print(f\"\\n3. TOP PREDICTIVE FEATURES (SHAP Analysis)\")\n",
    "for i, (idx, row) in enumerate(shap_importance.head(5).iterrows(), 1):\n",
    "    feature = row['Feature']\n",
    "    importance = row['Importance']\n",
    "    \n",
    "    # Get average SHAP value to determine direction\n",
    "    feature_idx = feature_cols.index(feature)\n",
    "    avg_shap = shap_values[:, feature_idx].mean()\n",
    "    direction = \"positive\" if avg_shap > 0 else \"negative\"\n",
    "    \n",
    "    print(f\"     {i}. {feature:20s} (importance: {importance:.4f}, {direction} effect)\")\n",
    "\n",
    "print(f\"\\n4. BUSINESS INSIGHTS\")\n",
    "print(f\"\\n   What Makes a Hit Song?\")\n",
    "print(f\"   Based on our model analysis:\")\n",
    "\n",
    "# Analyze top features\n",
    "for feature in shap_importance.head(3)['Feature']:\n",
    "    feature_idx = feature_cols.index(feature)\n",
    "    avg_shap = shap_values[:, feature_idx].mean()\n",
    "    \n",
    "    if avg_shap > 0:\n",
    "        print(f\"   â€¢ Higher {feature} â†’ More likely to be a hit\")\n",
    "    else:\n",
    "        print(f\"   â€¢ Lower {feature} â†’ More likely to be a hit\")\n",
    "\n",
    "print(f\"\\n5. MODEL LIMITATIONS\")\n",
    "print(f\"   â€¢ Dataset limited to Spotify audio features\")\n",
    "print(f\"   â€¢ Does not account for: marketing, artist fame, timing, luck\")\n",
    "print(f\"   â€¢ Class imbalance ({(y==0).sum()/(y==1).sum():.1f}:1) makes prediction challenging\")\n",
    "print(f\"   â€¢ Correlation â‰  causation (features associated with hits, not causing them)\")\n",
    "\n",
    "print(f\"\\n6. DELIVERABLES CREATED\")\n",
    "print(f\"\\n   Models:\")\n",
    "print(f\"     â€¢ {models_dir / 'final_xgboost.pkl'}\")\n",
    "print(f\"     â€¢ {models_dir / 'baseline_logreg.pkl'}\")\n",
    "print(f\"     â€¢ {models_dir / 'scaler.pkl'}\")\n",
    "\n",
    "print(f\"\\n   Visualizations:\")\n",
    "print(f\"     â€¢ {figures_dir / 'model_comparison.png'}\")\n",
    "print(f\"     â€¢ {figures_dir / 'xgboost_confusion_matrix.png'}\")\n",
    "print(f\"     â€¢ {figures_dir / 'shap_feature_importance.png'}\")\n",
    "print(f\"     â€¢ {figures_dir / 'shap_summary_detailed.png'}\")\n",
    "print(f\"     â€¢ {figures_dir / 'shap_dependence_plots.png'}\")\n",
    "\n",
    "print(f\"\\n   Data:\")\n",
    "print(f\"     â€¢ {processed_data_dir / 'hits_dataset.csv'}\")\n",
    "\n",
    "print(f\"\\n7. PRESENTATION RECOMMENDATIONS\")\n",
    "print(f\"\\n   Key slides to include:\")\n",
    "print(f\"   1. Problem: Predicting hit songs from audio features\")\n",
    "print(f\"   2. Dataset: {len(df):,} songs, {(df['is_hit']==1).sum():,} hits ({(df['is_hit']==1).sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"   3. Methodology: Logistic Regression â†’ XGBoost + SHAP\")\n",
    "print(f\"   4. Results: Show model_comparison.png\")\n",
    "print(f\"   5. Insights: Show shap_feature_importance.png\")\n",
    "print(f\"   6. Deep dive: Show shap_summary_detailed.png\")\n",
    "print(f\"   7. Limitations and future work\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… PROJECT COMPLETE - Ready for presentation!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Week 3 Complete - Project Finished!\n",
    "\n",
    "### What You've Accomplished:\n",
    "\n",
    "1. âœ… **Comprehensive Data Analysis**\n",
    "   - Loaded and cleaned Spotify dataset\n",
    "   - Created HIT labels through dataset matching\n",
    "   - Handled severe class imbalance\n",
    "\n",
    "2. âœ… **Two-Model Approach**\n",
    "   - Logistic Regression (interpretable baseline)\n",
    "   - XGBoost (high-performance non-linear model)\n",
    "\n",
    "3. âœ… **Advanced Interpretation**\n",
    "   - SHAP values for feature importance\n",
    "   - Dependence plots showing feature effects\n",
    "   - Individual prediction explanations\n",
    "\n",
    "4. âœ… **Professional Deliverables**\n",
    "   - Publication-quality visualizations\n",
    "   - Comprehensive evaluation metrics\n",
    "   - Saved models for deployment\n",
    "\n",
    "### For Your Presentation:\n",
    "\n",
    "Use these figures:\n",
    "- `model_comparison.png` - Show XGBoost improvements\n",
    "- `shap_feature_importance.png` - Main \"what makes a hit\" slide\n",
    "- `shap_summary_detailed.png` - Detailed feature effects\n",
    "- `correlation_matrix.png` - Initial data exploration\n",
    "\n",
    "### For Your Report:\n",
    "\n",
    "Structure:\n",
    "1. Introduction & Problem Statement\n",
    "2. Dataset & Methodology\n",
    "3. Exploratory Data Analysis (Week 1 figures)\n",
    "4. Baseline Model (Week 2 results)\n",
    "5. Advanced Model (Week 3 XGBoost)\n",
    "6. SHAP Interpretation\n",
    "7. Conclusions & Limitations\n",
    "\n",
    "### Next Steps (If Time Permits):\n",
    "\n",
    "- Try SMOTE for synthetic oversampling\n",
    "- Feature engineering (interaction terms, temporal features)\n",
    "- Try other models (Random Forest, Neural Networks)\n",
    "- Build a simple web app for predictions\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations on completing this project! ðŸŽ‰**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
