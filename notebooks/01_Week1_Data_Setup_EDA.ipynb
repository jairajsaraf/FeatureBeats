{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Data Setup and Exploratory Data Analysis\n",
    "\n",
    "## Objectives\n",
    "1. Load Spotify tracks dataset and Top 100 hits dataset\n",
    "2. Merge datasets and create HIT/NON-HIT labels\n",
    "3. Handle class imbalance\n",
    "4. Perform exploratory data analysis\n",
    "5. Visualize feature distributions\n",
    "6. Save processed dataset\n",
    "\n",
    "## Timeline\n",
    "**Week 1 (Nov 18-24)** - Due: Nov 24 (Checkpoint)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from fuzzywuzzy import fuzz\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "project_root = Path.cwd().parent\n",
    "data_dir = project_root / 'data'\n",
    "raw_data_dir = data_dir / 'raw'\n",
    "processed_data_dir = data_dir / 'processed'\n",
    "figures_dir = project_root / 'figures'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "processed_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Figures directory: {figures_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Datasets\n",
    "\n",
    "We'll load:\n",
    "1. **Main tracks dataset** - All Spotify tracks with audio features\n",
    "2. **Top 100 dataset** - Chart-topping hits (these will be labeled as HITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load main tracks dataset\n",
    "print(\"Loading main tracks dataset...\")\n",
    "tracks_file = raw_data_dir / 'tracks.csv'\n",
    "\n",
    "if not tracks_file.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"❌ tracks.csv not found at {tracks_file}\\n\"\n",
    "        \"Please download the Spotify Tracks dataset from Kaggle and place it in data/raw/\"\n",
    "    )\n",
    "\n",
    "tracks_df = pd.read_csv(tracks_file)\n",
    "print(f\"✅ Loaded {len(tracks_df):,} tracks\")\n",
    "print(f\"   Columns: {list(tracks_df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "tracks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Top 100 / Billboard hits dataset\n",
    "print(\"Loading Top 100 hits dataset...\")\n",
    "hits_file = raw_data_dir / 'top100_tracks.csv'\n",
    "\n",
    "if not hits_file.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"❌ top100_tracks.csv not found at {hits_file}\\n\"\n",
    "        \"Please download a Top 100/Billboard dataset from Kaggle and place it in data/raw/\"\n",
    "    )\n",
    "\n",
    "hits_df = pd.read_csv(hits_file)\n",
    "print(f\"✅ Loaded {len(hits_df):,} hit songs\")\n",
    "print(f\"   Columns: {list(hits_df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "hits_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "### 2.1 Identify Required Columns\n",
    "\n",
    "We need to identify which columns contain:\n",
    "- Track name\n",
    "- Artist name\n",
    "- Audio features (danceability, energy, etc.)\n",
    "- Year/release date\n",
    "- (Optional) Track ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect tracks dataset structure\n",
    "print(\"Tracks dataset info:\")\n",
    "print(tracks_df.info())\n",
    "print(\"\\nSample row:\")\n",
    "print(tracks_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect hits dataset structure\n",
    "print(\"Hits dataset info:\")\n",
    "print(hits_df.info())\n",
    "print(\"\\nSample row:\")\n",
    "print(hits_df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Standardize Column Names\n",
    "\n",
    "**Note:** The column names below are examples. You may need to adjust them based on your actual dataset.\n",
    "\n",
    "Common Spotify audio features:\n",
    "- `danceability`\n",
    "- `energy`\n",
    "- `key`\n",
    "- `loudness`\n",
    "- `mode`\n",
    "- `speechiness`\n",
    "- `acousticness`\n",
    "- `instrumentalness`\n",
    "- `liveness`\n",
    "- `valence`\n",
    "- `tempo`\n",
    "- `duration_ms`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define audio features to use (adjust based on your dataset)\n",
    "audio_features = [\n",
    "    'danceability', 'energy', 'loudness', 'speechiness',\n",
    "    'acousticness', 'instrumentalness', 'liveness', \n",
    "    'valence', 'tempo'\n",
    "]\n",
    "\n",
    "# Check which features are available\n",
    "available_features = [f for f in audio_features if f in tracks_df.columns]\n",
    "missing_features = [f for f in audio_features if f not in tracks_df.columns]\n",
    "\n",
    "print(f\"✅ Available features ({len(available_features)}): {available_features}\")\n",
    "if missing_features:\n",
    "    print(f\"⚠️  Missing features ({len(missing_features)}): {missing_features}\")\n",
    "\n",
    "# Use only available features\n",
    "audio_features = available_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Filter Data by Year Range\n",
    "\n",
    "Focus on a specific time period (e.g., 2010-2020) to ensure temporal relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define year range\n",
    "YEAR_START = 2010\n",
    "YEAR_END = 2020\n",
    "\n",
    "# Try to find year column (adjust column name as needed)\n",
    "year_col = None\n",
    "for col in ['year', 'release_year', 'release_date', 'album_release_date']:\n",
    "    if col in tracks_df.columns:\n",
    "        year_col = col\n",
    "        break\n",
    "\n",
    "if year_col:\n",
    "    print(f\"Using '{year_col}' column for year filtering\")\n",
    "    \n",
    "    # Extract year if it's a date string\n",
    "    if tracks_df[year_col].dtype == 'object':\n",
    "        tracks_df['year'] = pd.to_datetime(tracks_df[year_col], errors='coerce').dt.year\n",
    "    else:\n",
    "        tracks_df['year'] = tracks_df[year_col]\n",
    "    \n",
    "    # Filter by year range\n",
    "    tracks_filtered = tracks_df[\n",
    "        (tracks_df['year'] >= YEAR_START) & \n",
    "        (tracks_df['year'] <= YEAR_END)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"✅ Filtered to {YEAR_START}-{YEAR_END}: {len(tracks_filtered):,} tracks\")\n",
    "else:\n",
    "    print(\"⚠️  No year column found. Using all data.\")\n",
    "    tracks_filtered = tracks_df.copy()\n",
    "    tracks_filtered['year'] = 2015  # Default year\n",
    "\n",
    "print(f\"\\nYear distribution:\")\n",
    "print(tracks_filtered['year'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create HIT Labels\n",
    "\n",
    "We'll match songs from the Top 100 dataset with the main tracks dataset to create labels:\n",
    "- **HIT (1)** - Song appears in Top 100\n",
    "- **NON-HIT (0)** - Song does not appear in Top 100\n",
    "\n",
    "### Matching Strategy\n",
    "1. **Best:** Match by Spotify track ID (if available)\n",
    "2. **Good:** Exact match on (track name + artist name)\n",
    "3. **Fallback:** Fuzzy match on (track name + artist name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare hits dataset\n",
    "# Adjust column names based on your dataset\n",
    "print(\"Preparing hits dataset for matching...\")\n",
    "\n",
    "# Try to find track name and artist columns in hits dataset\n",
    "track_name_col = None\n",
    "artist_col = None\n",
    "\n",
    "for col in ['track_name', 'title', 'song', 'name']:\n",
    "    if col in hits_df.columns:\n",
    "        track_name_col = col\n",
    "        break\n",
    "\n",
    "for col in ['artist', 'artist_name', 'artists']:\n",
    "    if col in hits_df.columns:\n",
    "        artist_col = col\n",
    "        break\n",
    "\n",
    "if not track_name_col or not artist_col:\n",
    "    print(\"⚠️  Could not auto-detect column names. Please set them manually:\")\n",
    "    print(f\"   Available columns: {list(hits_df.columns)}\")\n",
    "    # Uncomment and set manually if needed:\n",
    "    # track_name_col = 'your_track_column'\n",
    "    # artist_col = 'your_artist_column'\n",
    "else:\n",
    "    print(f\"✅ Using track column: '{track_name_col}'\")\n",
    "    print(f\"✅ Using artist column: '{artist_col}'\")\n",
    "\n",
    "# Standardize hits dataset\n",
    "hits_standardized = hits_df[[track_name_col, artist_col]].copy()\n",
    "hits_standardized.columns = ['track_name', 'artist']\n",
    "\n",
    "# Clean text for better matching\n",
    "hits_standardized['track_name'] = hits_standardized['track_name'].str.lower().str.strip()\n",
    "hits_standardized['artist'] = hits_standardized['artist'].str.lower().str.strip()\n",
    "\n",
    "print(f\"\\nPrepared {len(hits_standardized):,} hit songs for matching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare main tracks for matching\n",
    "print(\"Preparing tracks dataset for matching...\")\n",
    "\n",
    "# Find track name and artist columns in tracks dataset\n",
    "track_col_tracks = None\n",
    "artist_col_tracks = None\n",
    "\n",
    "for col in ['track_name', 'title', 'song', 'name']:\n",
    "    if col in tracks_filtered.columns:\n",
    "        track_col_tracks = col\n",
    "        break\n",
    "\n",
    "for col in ['artist', 'artist_name', 'artists']:\n",
    "    if col in tracks_filtered.columns:\n",
    "        artist_col_tracks = col\n",
    "        break\n",
    "\n",
    "if not track_col_tracks or not artist_col_tracks:\n",
    "    print(\"⚠️  Could not auto-detect column names. Please set them manually:\")\n",
    "    print(f\"   Available columns: {list(tracks_filtered.columns)}\")\n",
    "else:\n",
    "    print(f\"✅ Using track column: '{track_col_tracks}'\")\n",
    "    print(f\"✅ Using artist column: '{artist_col_tracks}'\")\n",
    "\n",
    "# Create standardized matching columns\n",
    "tracks_filtered['track_clean'] = tracks_filtered[track_col_tracks].str.lower().str.strip()\n",
    "tracks_filtered['artist_clean'] = tracks_filtered[artist_col_tracks].str.lower().str.strip()\n",
    "tracks_filtered['match_key'] = tracks_filtered['track_clean'] + '___' + tracks_filtered['artist_clean']\n",
    "\n",
    "# Create match key for hits\n",
    "hits_standardized['match_key'] = hits_standardized['track_name'] + '___' + hits_standardized['artist']\n",
    "hit_keys = set(hits_standardized['match_key'].unique())\n",
    "\n",
    "print(f\"Created {len(hit_keys):,} unique hit song identifiers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HIT labels using exact matching\n",
    "print(\"Creating HIT labels...\\n\")\n",
    "\n",
    "tracks_filtered['is_hit'] = tracks_filtered['match_key'].isin(hit_keys).astype(int)\n",
    "\n",
    "# Check class balance\n",
    "hit_counts = tracks_filtered['is_hit'].value_counts()\n",
    "total = len(tracks_filtered)\n",
    "\n",
    "print(\"Class Distribution:\")\n",
    "print(f\"  NON-HIT (0): {hit_counts.get(0, 0):,} ({hit_counts.get(0, 0)/total*100:.2f}%)\")\n",
    "print(f\"  HIT (1):     {hit_counts.get(1, 0):,} ({hit_counts.get(1, 0)/total*100:.2f}%)\")\n",
    "print(f\"\\n  Total tracks: {total:,}\")\n",
    "print(f\"  Imbalance ratio: {hit_counts.get(0, 0) / max(hit_counts.get(1, 1), 1):.1f}:1\")\n",
    "\n",
    "if hit_counts.get(1, 0) < 50:\n",
    "    print(\"\\n⚠️  WARNING: Very few hits matched. You may need to:\")\n",
    "    print(\"   1. Check if column names are correct\")\n",
    "    print(\"   2. Use fuzzy matching (see next cell)\")\n",
    "    print(\"   3. Try a different hits dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Fuzzy Matching (if exact matching found too few hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if you need fuzzy matching\n",
    "\n",
    "# def fuzzy_match_song(track, artist, hits_df, threshold=85):\n",
    "#     \"\"\"Find best fuzzy match for a song in hits dataset\"\"\"\n",
    "#     best_score = 0\n",
    "#     for _, hit in hits_df.iterrows():\n",
    "#         # Score based on both track name and artist\n",
    "#         track_score = fuzz.ratio(track.lower(), hit['track_name'].lower())\n",
    "#         artist_score = fuzz.ratio(artist.lower(), hit['artist'].lower())\n",
    "#         combined_score = (track_score + artist_score) / 2\n",
    "#         \n",
    "#         if combined_score > best_score:\n",
    "#             best_score = combined_score\n",
    "#     \n",
    "#     return best_score >= threshold\n",
    "\n",
    "# # Apply fuzzy matching (this may take a while for large datasets)\n",
    "# print(\"Applying fuzzy matching... (this may take several minutes)\")\n",
    "# \n",
    "# # Only check songs that weren't already matched\n",
    "# non_matched = tracks_filtered[tracks_filtered['is_hit'] == 0].copy()\n",
    "# \n",
    "# # Sample if dataset is too large\n",
    "# if len(non_matched) > 10000:\n",
    "#     print(f\"Sampling 10,000 tracks for fuzzy matching (dataset too large)\")\n",
    "#     non_matched = non_matched.sample(10000, random_state=RANDOM_SEED)\n",
    "# \n",
    "# fuzzy_hits = []\n",
    "# for idx, row in tqdm(non_matched.iterrows(), total=len(non_matched)):\n",
    "#     if fuzzy_match_song(row[track_col_tracks], row[artist_col_tracks], hits_standardized):\n",
    "#         fuzzy_hits.append(idx)\n",
    "# \n",
    "# # Update labels\n",
    "# tracks_filtered.loc[fuzzy_hits, 'is_hit'] = 1\n",
    "# \n",
    "# print(f\"\\n✅ Found {len(fuzzy_hits)} additional hits using fuzzy matching\")\n",
    "# \n",
    "# # Show updated distribution\n",
    "# print(\"\\nUpdated class distribution:\")\n",
    "# print(tracks_filtered['is_hit'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean and Prepare Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns for modeling\n",
    "columns_to_keep = audio_features + ['is_hit', 'year']\n",
    "\n",
    "# Add track info for reference (not for modeling)\n",
    "if track_col_tracks in tracks_filtered.columns:\n",
    "    columns_to_keep.append(track_col_tracks)\n",
    "if artist_col_tracks in tracks_filtered.columns:\n",
    "    columns_to_keep.append(artist_col_tracks)\n",
    "\n",
    "# Create final dataset\n",
    "df_final = tracks_filtered[columns_to_keep].copy()\n",
    "\n",
    "# Handle missing values\n",
    "print(\"Missing values per column:\")\n",
    "missing = df_final.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(missing[missing > 0])\n",
    "    print(\"\\nDropping rows with missing values...\")\n",
    "    df_final = df_final.dropna()\n",
    "    print(f\"✅ Removed {len(tracks_filtered) - len(df_final)} rows\")\n",
    "else:\n",
    "    print(\"✅ No missing values found\")\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {df_final.shape}\")\n",
    "print(f\"Features: {audio_features}\")\n",
    "print(f\"Target: is_hit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final dataset info\n",
    "print(\"Final dataset summary:\")\n",
    "df_final.info()\n",
    "print(\"\\nFirst few rows:\")\n",
    "df_final.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis\n",
    "\n",
    "### 5.1 Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Summary statistics for audio features:\\n\")\n",
    "df_final[audio_features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary by class\n",
    "print(\"Summary statistics by class:\\n\")\n",
    "print(\"NON-HITS (0):\")\n",
    "print(df_final[df_final['is_hit'] == 0][audio_features].describe())\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(\"HITS (1):\")\n",
    "print(df_final[df_final['is_hit'] == 1][audio_features].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Visualizations\n",
    "\n",
    "#### Distribution of Tracks by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracks by year\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "year_counts = df_final.groupby(['year', 'is_hit']).size().unstack(fill_value=0)\n",
    "year_counts.plot(kind='bar', stacked=True, ax=ax, color=['#3498db', '#e74c3c'])\n",
    "\n",
    "ax.set_xlabel('Year', fontsize=12)\n",
    "ax.set_ylabel('Number of Tracks', fontsize=12)\n",
    "ax.set_title('Distribution of Tracks by Year and Hit Status', fontsize=14, fontweight='bold')\n",
    "ax.legend(['Non-Hit', 'Hit'], title='Class')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(figures_dir / 'tracks_by_year.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Saved: tracks_by_year.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Distributions: Hits vs Non-Hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for each feature\n",
    "n_features = len(audio_features)\n",
    "n_cols = 3\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten() if n_features > 1 else [axes]\n",
    "\n",
    "for idx, feature in enumerate(audio_features):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Create box plot\n",
    "    data_to_plot = [\n",
    "        df_final[df_final['is_hit'] == 0][feature],\n",
    "        df_final[df_final['is_hit'] == 1][feature]\n",
    "    ]\n",
    "    \n",
    "    bp = ax.boxplot(data_to_plot, labels=['Non-Hit', 'Hit'], \n",
    "                     patch_artist=True, showmeans=True)\n",
    "    \n",
    "    # Color the boxes\n",
    "    bp['boxes'][0].set_facecolor('#3498db')\n",
    "    bp['boxes'][1].set_facecolor('#e74c3c')\n",
    "    \n",
    "    ax.set_ylabel(feature.capitalize(), fontsize=11)\n",
    "    ax.set_title(f'{feature.capitalize()} Distribution', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide extra subplots\n",
    "for idx in range(n_features, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.suptitle('Audio Feature Distributions: Hits vs Non-Hits', \n",
    "             fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(figures_dir / 'feature_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Saved: feature_distributions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Calculate correlation\n",
    "corr_matrix = df_final[audio_features + ['is_hit']].corr()\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8},\n",
    "            ax=ax)\n",
    "\n",
    "ax.set_title('Feature Correlation Matrix', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(figures_dir / 'correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Saved: correlation_matrix.png\")\n",
    "print(\"\\nCorrelation with 'is_hit':\")\n",
    "print(corr_matrix['is_hit'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed dataset\n",
    "output_file = processed_data_dir / 'hits_dataset.csv'\n",
    "df_final.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"✅ Saved processed dataset to: {output_file}\")\n",
    "print(f\"   Shape: {df_final.shape}\")\n",
    "print(f\"   Features: {len(audio_features)}\")\n",
    "print(f\"   Samples: {len(df_final):,}\")\n",
    "print(f\"   Hits: {(df_final['is_hit'] == 1).sum():,}\")\n",
    "print(f\"   Non-hits: {(df_final['is_hit'] == 0).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Findings Summary\n",
    "\n",
    "### Data Overview\n",
    "- **Total tracks**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "print(\"=\"*80)\n",
    "print(\"WEEK 1 SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. DATASET STATISTICS\")\n",
    "print(f\"   Total tracks: {len(df_final):,}\")\n",
    "print(f\"   Hit songs: {(df_final['is_hit'] == 1).sum():,} ({(df_final['is_hit'] == 1).sum()/len(df_final)*100:.2f}%)\")\n",
    "print(f\"   Non-hit songs: {(df_final['is_hit'] == 0).sum():,} ({(df_final['is_hit'] == 0).sum()/len(df_final)*100:.2f}%)\")\n",
    "print(f\"   Year range: {df_final['year'].min():.0f} - {df_final['year'].max():.0f}\")\n",
    "\n",
    "print(f\"\\n2. CLASS IMBALANCE\")\n",
    "ratio = (df_final['is_hit'] == 0).sum() / max((df_final['is_hit'] == 1).sum(), 1)\n",
    "print(f\"   Imbalance ratio: {ratio:.1f}:1 (non-hits : hits)\")\n",
    "if ratio > 10:\n",
    "    print(f\"   ⚠️  SEVERE IMBALANCE - Will need class weighting or SMOTE\")\n",
    "elif ratio > 5:\n",
    "    print(f\"   ⚠️  MODERATE IMBALANCE - Will use class weighting\")\n",
    "else:\n",
    "    print(f\"   ✅ Manageable imbalance\")\n",
    "\n",
    "print(f\"\\n3. FEATURE CORRELATIONS WITH HIT STATUS\")\n",
    "print(\"   Top 5 positive correlations:\")\n",
    "correlations = corr_matrix['is_hit'].drop('is_hit').sort_values(ascending=False)\n",
    "for i, (feature, corr) in enumerate(correlations.head().items(), 1):\n",
    "    print(f\"   {i}. {feature}: {corr:+.4f}\")\n",
    "\n",
    "print(f\"\\n   Top 5 negative correlations:\")\n",
    "for i, (feature, corr) in enumerate(correlations.tail().items(), 1):\n",
    "    print(f\"   {i}. {feature}: {corr:+.4f}\")\n",
    "\n",
    "print(f\"\\n4. NEXT STEPS (Week 2)\")\n",
    "print(f\"   → Build logistic regression baseline\")\n",
    "print(f\"   → Implement class weighting\")\n",
    "print(f\"   → Evaluate with appropriate metrics (F1, Recall, PR-AUC)\")\n",
    "print(f\"   → Interpret coefficients\")\n",
    "\n",
    "print(f\"\\n5. FILES CREATED\")\n",
    "print(f\"   → {output_file}\")\n",
    "print(f\"   → {figures_dir / 'tracks_by_year.png'}\")\n",
    "print(f\"   → {figures_dir / 'feature_distributions.png'}\")\n",
    "print(f\"   → {figures_dir / 'correlation_matrix.png'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ WEEK 1 COMPLETE - Ready for modeling!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ✅ Week 1 Deliverables Complete!\n",
    "\n",
    "You now have:\n",
    "1. ✅ Cleaned and labeled dataset\n",
    "2. ✅ Understanding of class imbalance\n",
    "3. ✅ Visualizations showing feature distributions\n",
    "4. ✅ Correlation analysis\n",
    "\n",
    "### For Checkpoint Presentation (Nov 24):\n",
    "Use these figures:\n",
    "- `tracks_by_year.png` - Show dataset scope\n",
    "- `feature_distributions.png` - Show how hits differ from non-hits\n",
    "- `correlation_matrix.png` - Highlight most correlated features\n",
    "\n",
    "### Next Steps:\n",
    "Proceed to **02_Week2_Baseline_Modeling.ipynb** to build the logistic regression baseline.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
