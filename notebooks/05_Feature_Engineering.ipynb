{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Hit Song Prediction\n",
    "\n",
    "## Objectives\n",
    "1. Create interaction features (e.g., energy √ó danceability)\n",
    "2. Add polynomial features for non-linear relationships\n",
    "3. Engineer temporal features (month, day of week)\n",
    "4. Create domain-specific features\n",
    "5. Evaluate impact on model performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, roc_auc_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "processed_data_dir = project_root / 'data' / 'processed'\n",
    "figures_dir = project_root / 'figures'\n",
    "\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the processed dataset\ndata_file = processed_data_dir / 'hits_dataset.csv'\n\nif not data_file.exists():\n    print(\"‚ùå ERROR: hits_dataset.csv not found!\")\n    print(f\"   Expected location: {data_file}\")\n    print(\"\\nüìã Please run the following notebooks first:\")\n    print(\"   1. 01_Week1_Data_Setup_EDA.ipynb\")\n    print(\"   2. 02_Week2_Baseline_Modeling.ipynb\")\n    print(\"\\nThese notebooks will create the hits_dataset.csv file needed for feature engineering.\")\n    raise FileNotFoundError(f\"Required file not found: {data_file}\")\n\ndf = pd.read_csv(data_file)\nprint(f\"‚úÖ Dataset loaded: {df.shape}\")\nprint(f\"   Features: {df.shape[1]}\")\nprint(f\"   Samples: {df.shape[0]}\")\ndf.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Interaction Features\n",
    "\n",
    "Combine features that might work together to predict hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get audio feature columns\n",
    "exclude_cols = ['is_hit', 'year'] + df.select_dtypes(include=['object']).columns.tolist()\n",
    "audio_features = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "# Create interaction features\n",
    "df_engineered = df.copy()\n",
    "\n",
    "# Domain knowledge interactions\n",
    "if 'energy' in audio_features and 'danceability' in audio_features:\n",
    "    df_engineered['energy_x_danceability'] = df['energy'] * df['danceability']\n",
    "    print(\"‚úÖ Created: energy √ó danceability\")\n",
    "\n",
    "if 'valence' in audio_features and 'energy' in audio_features:\n",
    "    df_engineered['valence_x_energy'] = df['valence'] * df['energy']\n",
    "    print(\"‚úÖ Created: valence √ó energy (happy & energetic)\")\n",
    "\n",
    "if 'loudness' in audio_features and 'energy' in audio_features:\n",
    "    df_engineered['loudness_x_energy'] = df['loudness'] * df['energy']\n",
    "    print(\"‚úÖ Created: loudness √ó energy\")\n",
    "\n",
    "if 'acousticness' in audio_features and 'energy' in audio_features:\n",
    "    df_engineered['acoustic_vs_energy'] = df['acousticness'] - df['energy']\n",
    "    print(\"‚úÖ Created: acousticness - energy (acoustic contrast)\")\n",
    "\n",
    "# Danceability composite\n",
    "if all(f in audio_features for f in ['danceability', 'valence', 'energy']):\n",
    "    df_engineered['party_factor'] = (df['danceability'] + df['valence'] + df['energy']) / 3\n",
    "    print(\"‚úÖ Created: party_factor (avg of dance, valence, energy)\")\n",
    "\n",
    "print(f\"\\nDataset shape after interactions: {df_engineered.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Polynomial Features\n",
    "\n",
    "Capture non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add squared terms for key features\n",
    "key_features = ['danceability', 'energy', 'valence'] if all(f in audio_features for f in ['danceability', 'energy', 'valence']) else audio_features[:3]\n",
    "\n",
    "for feature in key_features:\n",
    "    if feature in df.columns:\n",
    "        df_engineered[f'{feature}_squared'] = df[feature] ** 2\n",
    "        print(f\"‚úÖ Created: {feature}¬≤\")\n",
    "\n",
    "print(f\"\\nDataset shape after polynomial features: {df_engineered.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Temporal Features\n",
    "\n",
    "Extract month, season, day of week if date information is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Year-based features\nif 'year' in df.columns:\n    year_min = df['year'].min()\n    year_max = df['year'].max()\n    \n    # Normalize year (0-1 scale)\n    if year_max > year_min:\n        df_engineered['year_normalized'] = (df['year'] - year_min) / (year_max - year_min)\n    else:\n        # All years are the same, set to 0.5\n        df_engineered['year_normalized'] = 0.5\n        print(\"‚ö†Ô∏è  Warning: All songs from the same year, year_normalized set to 0.5\")\n    \n    # Year bins (early, mid, late period)\n    try:\n        if year_max - year_min >= 2:  # Need at least 3 distinct values for 3 bins\n            df_engineered['year_period'] = pd.cut(df['year'], bins=3, labels=[0, 1, 2]).astype(int)\n        else:\n            # Not enough year range for binning\n            df_engineered['year_period'] = 1  # Set all to middle period\n            print(f\"‚ö†Ô∏è  Warning: Year range too small ({year_min}-{year_max}), year_period set to 1\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è  Warning: Could not create year_period bins: {e}\")\n        df_engineered['year_period'] = 1\n    \n    print(\"‚úÖ Created: year_normalized, year_period\")\nelse:\n    print(\"‚ö†Ô∏è  No 'year' column found, skipping temporal features\")\n\nprint(f\"\\nFinal engineered dataset shape: {df_engineered.shape}\")\nprint(f\"Added {df_engineered.shape[1] - df.shape[1]} new features\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "exclude_cols = ['is_hit', 'year'] + df_engineered.select_dtypes(include=['object']).columns.tolist()\n",
    "feature_cols = [col for col in df_engineered.columns if col not in exclude_cols]\n",
    "\n",
    "X = df_engineered[feature_cols].values\n",
    "y = df_engineered['is_hit'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Original vs Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get original features for comparison\noriginal_features = [col for col in df.columns if col not in ['is_hit', 'year'] and col not in df.select_dtypes(include=['object']).columns]\n\n# Use the SAME train/test split for fair comparison\n# Create stratified split indices to ensure both models use identical train/test sets\nX_orig = df[original_features].values\nindices = np.arange(len(y))\ntrain_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=RANDOM_SEED, stratify=y)\n\n# Apply same split to original features\nX_orig_train = X_orig[train_idx]\nX_orig_test = X_orig[test_idx]\ny_orig_train = y[train_idx]\ny_orig_test = y[test_idx]\n\n# Scale original features\nscaler_orig = StandardScaler()\nX_orig_train_scaled = scaler_orig.fit_transform(X_orig_train)\nX_orig_test_scaled = scaler_orig.transform(X_orig_test)\n\n# Train original features model\nmodel_orig = LogisticRegression(class_weight='balanced', random_state=RANDOM_SEED, max_iter=1000)\nmodel_orig.fit(X_orig_train_scaled, y_orig_train)\ny_pred_orig = model_orig.predict(X_orig_test_scaled)\n\n# Train engineered features model (already done in previous cells, but ensuring same split)\nmodel_eng = LogisticRegression(class_weight='balanced', random_state=RANDOM_SEED, max_iter=1000)\nmodel_eng.fit(X_train_scaled, y_train)\ny_pred_eng = model_eng.predict(X_test_scaled)\n\n# Verify same test set (should be True)\nassert np.array_equal(y_orig_test, y_test), \"Test sets don't match! Check random seed.\"\n\n# Compare metrics on the SAME test set\nprint(\"\\n\" + \"=\"*60)\nprint(\"ORIGINAL vs ENGINEERED FEATURES COMPARISON\")\nprint(\"=\"*60)\n\nmetrics = {\n    'Features Count': [len(original_features), len(feature_cols)],\n    'Precision': [precision_score(y_orig_test, y_pred_orig), precision_score(y_test, y_pred_eng)],\n    'Recall': [recall_score(y_orig_test, y_pred_orig), recall_score(y_test, y_pred_eng)],\n    'F1 Score': [f1_score(y_orig_test, y_pred_orig), f1_score(y_test, y_pred_eng)]\n}\n\ncomparison = pd.DataFrame(metrics, index=['Original', 'Engineered'])\nprint(comparison)\n\nimprovement = ((comparison.loc['Engineered', 'F1 Score'] - comparison.loc['Original', 'F1 Score']) /\n               comparison.loc['Original', 'F1 Score'] * 100)\nprint(f\"\\nF1 Score Improvement: {improvement:+.2f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Top Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from coefficients\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Coefficient': model_eng.coef_[0],\n",
    "    'Abs_Coefficient': np.abs(model_eng.coef_[0])\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(\"=\"*60)\n",
    "for idx, row in feature_importance.head(10).iterrows():\n",
    "    feature_type = \"ENGINEERED\" if row['Feature'] not in original_features else \"ORIGINAL\"\n",
    "    print(f\"{row['Feature']:30s} {row['Coefficient']:+.4f}  [{feature_type}]\")\n",
    "\n",
    "# Count engineered features in top 10\n",
    "top10_engineered = sum(1 for f in feature_importance.head(10)['Feature'] if f not in original_features)\n",
    "print(f\"\\n{top10_engineered}/10 top features are engineered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Engineered Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Ensure output directory exists\nprocessed_data_dir.mkdir(parents=True, exist_ok=True)\n\n# Save engineered dataset\noutput_file = processed_data_dir / 'hits_dataset_engineered.csv'\ndf_engineered.to_csv(output_file, index=False)\n\nprint(f\"‚úÖ Saved engineered dataset to: {output_file}\")\nprint(f\"   Original features: {len(original_features)}\")\nprint(f\"   Total features: {len(feature_cols)}\")\nprint(f\"   New features: {len(feature_cols) - len(original_features)}\")\nprint(f\"\\nüìä Dataset info:\")\nprint(f\"   Rows: {df_engineered.shape[0]:,}\")\nprint(f\"   Columns: {df_engineered.shape[1]}\")\nprint(f\"   File size: {output_file.stat().st_size / 1024:.1f} KB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Feature Engineering Complete!\n",
    "\n",
    "### New Features Created:\n",
    "1. **Interaction Terms**: energy√ódanceability, valence√óenergy, etc.\n",
    "2. **Polynomial Features**: Squared terms for key features\n",
    "3. **Domain Features**: party_factor, acoustic_contrast\n",
    "4. **Temporal Features**: year_normalized, year_period\n",
    "\n",
    "### Impact:\n",
    "- Original features: Basic Spotify audio features\n",
    "- Engineered features: Enhanced with domain knowledge\n",
    "- Can improve model performance by capturing complex patterns\n",
    "\n",
    "### Usage:\n",
    "Use `hits_dataset_engineered.csv` in subsequent modeling for potentially better results!\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}