{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Hit Song Prediction\n",
    "\n",
    "## Objectives\n",
    "1. Create interaction features (e.g., energy × danceability)\n",
    "2. Add polynomial features for non-linear relationships\n",
    "3. Engineer temporal features (month, day of week)\n",
    "4. Create domain-specific features\n",
    "5. Evaluate impact on model performance\n",
    "\n",
    "\n",
    "In this notebook, we enrich the original Spotify dataset by adding interaction terms, polynomial features and simple temporal features to capture relationships the baseline model might miss. After creating features like energy x danceability, valence x energy, squared terms and a “party factor,” we rebuild the dataset, apply the same train–test split and scaling, and train a logistic regression model with class weighting. We then compare this engineered feature model directly to the original feature set to see whether the new features improve predictive power, and identify which engineered features matter most. The final engineered dataset is saved for use in later modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, roc_auc_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "processed_data_dir = project_root / 'data' / 'processed'\n",
    "figures_dir = project_root / 'figures'\n",
    "\n",
    "print(\"Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: (113999, 13)\n",
      "   Features: 13\n",
      "   Samples: 113999\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>is_hit</th>\n",
       "      <th>year</th>\n",
       "      <th>track_name</th>\n",
       "      <th>artists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.676</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>-6.746</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>0.715</td>\n",
       "      <td>87.917</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Gen Hoshino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.420</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>-17.235</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.267</td>\n",
       "      <td>77.489</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>Ghost - Acoustic</td>\n",
       "      <td>Ben Woodward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.438</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>-9.734</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.120</td>\n",
       "      <td>76.332</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>To Begin Again</td>\n",
       "      <td>Ingrid Michaelson;ZAYN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.266</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>-18.515</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.143</td>\n",
       "      <td>181.740</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>Can't Help Falling In Love</td>\n",
       "      <td>Kina Grannis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.618</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>-9.681</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.167</td>\n",
       "      <td>119.949</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>Hold On</td>\n",
       "      <td>Chord Overstreet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   danceability  energy  loudness  speechiness  acousticness  \\\n",
       "0         0.676  0.4610    -6.746       0.1430        0.0322   \n",
       "1         0.420  0.1660   -17.235       0.0763        0.9240   \n",
       "2         0.438  0.3590    -9.734       0.0557        0.2100   \n",
       "3         0.266  0.0596   -18.515       0.0363        0.9050   \n",
       "4         0.618  0.4430    -9.681       0.0526        0.4690   \n",
       "\n",
       "   instrumentalness  liveness  valence    tempo  is_hit  year  \\\n",
       "0          0.000001    0.3580    0.715   87.917       0  2015   \n",
       "1          0.000006    0.1010    0.267   77.489       0  2015   \n",
       "2          0.000000    0.1170    0.120   76.332       0  2015   \n",
       "3          0.000071    0.1320    0.143  181.740       0  2015   \n",
       "4          0.000000    0.0829    0.167  119.949       1  2015   \n",
       "\n",
       "                   track_name                 artists  \n",
       "0                      Comedy             Gen Hoshino  \n",
       "1            Ghost - Acoustic            Ben Woodward  \n",
       "2              To Begin Again  Ingrid Michaelson;ZAYN  \n",
       "3  Can't Help Falling In Love            Kina Grannis  \n",
       "4                     Hold On        Chord Overstreet  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the processed dataset\n",
    "data_file = processed_data_dir / 'hits_dataset.csv'\n",
    "\n",
    "if not data_file.exists():\n",
    "    print(\"ERROR: hits_dataset.csv not found!\")\n",
    "    print(f\"   Expected location: {data_file}\")\n",
    "    print(\"\\nPlease run the following notebooks first:\")\n",
    "    print(\"   1. 01_Week1_Data_Setup_EDA.ipynb\")\n",
    "    print(\"   2. 02_Week2_Baseline_Modeling.ipynb\")\n",
    "    print(\"\\nThese notebooks will create the hits_dataset.csv file needed for feature engineering.\")\n",
    "    raise FileNotFoundError(f\"Required file not found: {data_file}\")\n",
    "\n",
    "df = pd.read_csv(data_file)\n",
    "print(f\"Dataset loaded: {df.shape}\")\n",
    "print(f\"   Features: {df.shape[1]}\")\n",
    "print(f\"   Samples: {df.shape[0]}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Interaction Features\n",
    "\n",
    "Combine features that might work together to predict hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: energy × danceability\n",
      "Created: valence × energy (happy & energetic)\n",
      "Created: loudness × energy\n",
      "Created: acousticness - energy (acoustic contrast)\n",
      "Created: party_factor (avg of dance, valence, energy)\n",
      "\n",
      "Dataset shape after interactions: (113999, 18)\n"
     ]
    }
   ],
   "source": [
    "# Get audio feature columns\n",
    "exclude_cols = ['is_hit', 'year'] + df.select_dtypes(include=['object']).columns.tolist()\n",
    "audio_features = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "# Create interaction features\n",
    "df_engineered = df.copy()\n",
    "\n",
    "# Domain knowledge interactions\n",
    "if 'energy' in audio_features and 'danceability' in audio_features:\n",
    "    df_engineered['energy_x_danceability'] = df['energy'] * df['danceability']\n",
    "    print(\"Created: energy × danceability\")\n",
    "\n",
    "if 'valence' in audio_features and 'energy' in audio_features:\n",
    "    df_engineered['valence_x_energy'] = df['valence'] * df['energy']\n",
    "    print(\"Created: valence × energy (happy & energetic)\")\n",
    "if 'loudness' in audio_features and 'energy' in audio_features:\n",
    "    df_engineered['loudness_x_energy'] = df['loudness'] * df['energy']\n",
    "    print(\"Created: loudness × energy\")\n",
    "\n",
    "if 'acousticness' in audio_features and 'energy' in audio_features:\n",
    "    df_engineered['acoustic_vs_energy'] = df['acousticness'] - df['energy']\n",
    "    print(\"Created: acousticness - energy (acoustic contrast)\")\n",
    "# Danceability composite\n",
    "if all(f in audio_features for f in ['danceability', 'valence', 'energy']):\n",
    "    df_engineered['party_factor'] = (df['danceability'] + df['valence'] + df['energy']) / 3\n",
    "    print(\"Created: party_factor (avg of dance, valence, energy)\")\n",
    "print(f\"\\nDataset shape after interactions: {df_engineered.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Polynomial Features\n",
    "\n",
    "Capture non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: danceability²\n",
      "Created: energy²\n",
      "Created: valence²\n",
      "\n",
      "Dataset shape after polynomial features: (113999, 21)\n"
     ]
    }
   ],
   "source": [
    "# Add squared terms for key features\n",
    "key_features = ['danceability', 'energy', 'valence'] if all(f in audio_features for f in ['danceability', 'energy', 'valence']) else audio_features[:3]\n",
    "\n",
    "for feature in key_features:\n",
    "    if feature in df.columns:\n",
    "        df_engineered[f'{feature}_squared'] = df[feature] ** 2\n",
    "        print(f\"Created: {feature}²\")\n",
    "\n",
    "print(f\"\\nDataset shape after polynomial features: {df_engineered.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Temporal Features\n",
    "\n",
    "Extract month, season, day of week if date information is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: All songs from the same year, year_normalized set to 0.5\n",
      "Warning: Year range too small (2015-2015), year_period set to 1\n",
      "Created: year_normalized, year_period\n",
      "\n",
      "Final engineered dataset shape: (113999, 23)\n",
      "Added 10 new features\n"
     ]
    }
   ],
   "source": [
    "# Year-based features\n",
    "if 'year' in df.columns:\n",
    "    year_min = df['year'].min()\n",
    "    year_max = df['year'].max()\n",
    "    \n",
    "    # Normalize year (0-1 scale)\n",
    "    if year_max > year_min:\n",
    "        df_engineered['year_normalized'] = (df['year'] - year_min) / (year_max - year_min)\n",
    "    else:\n",
    "        # All years are the same, set to 0.5\n",
    "        df_engineered['year_normalized'] = 0.5\n",
    "        print(\"Warning: All songs from the same year, year_normalized set to 0.5\")\n",
    "    \n",
    "    # Year bins (early, mid, late period)\n",
    "    try:\n",
    "        if year_max - year_min >= 2:  # Need at least 3 distinct values for 3 bins\n",
    "            df_engineered['year_period'] = pd.cut(df['year'], bins=3, labels=[0, 1, 2]).astype(int)\n",
    "        else:\n",
    "            # Not enough year range for binning\n",
    "            df_engineered['year_period'] = 1  # Set all to middle period\n",
    "            print(f\"Warning: Year range too small ({year_min}-{year_max}), year_period set to 1\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create year_period bins: {e}\")\n",
    "        df_engineered['year_period'] = 1\n",
    "    \n",
    "    print(\"Created: year_normalized, year_period\")\n",
    "else:\n",
    "    print(\"No 'year' column found, skipping temporal features\")\n",
    "\n",
    "print(f\"\\nFinal engineered dataset shape: {df_engineered.shape}\")\n",
    "print(f\"Added {df_engineered.shape[1] - df.shape[1]} new features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (91199, 19)\n",
      "Features: 19\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "exclude_cols = ['is_hit', 'year'] + df_engineered.select_dtypes(include=['object']).columns.tolist()\n",
    "feature_cols = [col for col in df_engineered.columns if col not in exclude_cols]\n",
    "\n",
    "X = df_engineered[feature_cols].values\n",
    "y = df_engineered['is_hit'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Original vs Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ORIGINAL vs ENGINEERED FEATURES COMPARISON\n",
      "============================================================\n",
      "            Features Count  Precision    Recall  F1 Score\n",
      "Original                 9   0.039352  0.822430  0.075109\n",
      "Engineered              19   0.040371  0.824766  0.076973\n",
      "\n",
      "F1 Score Improvement: +2.48%\n"
     ]
    }
   ],
   "source": [
    "# Get original features for comparison\n",
    "original_features = [col for col in df.columns if col not in ['is_hit', 'year'] and col not in df.select_dtypes(include=['object']).columns]\n",
    "\n",
    "# Use the SAME train/test split for fair comparison\n",
    "# Create stratified split indices to ensure both models use identical train/test sets\n",
    "X_orig = df[original_features].values\n",
    "indices = np.arange(len(y))\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=RANDOM_SEED, stratify=y)\n",
    "\n",
    "# Apply same split to original features\n",
    "X_orig_train = X_orig[train_idx]\n",
    "X_orig_test = X_orig[test_idx]\n",
    "y_orig_train = y[train_idx]\n",
    "y_orig_test = y[test_idx]\n",
    "\n",
    "# Scale original features\n",
    "scaler_orig = StandardScaler()\n",
    "X_orig_train_scaled = scaler_orig.fit_transform(X_orig_train)\n",
    "X_orig_test_scaled = scaler_orig.transform(X_orig_test)\n",
    "\n",
    "# Train original features model\n",
    "model_orig = LogisticRegression(class_weight='balanced', random_state=RANDOM_SEED, max_iter=1000)\n",
    "model_orig.fit(X_orig_train_scaled, y_orig_train)\n",
    "y_pred_orig = model_orig.predict(X_orig_test_scaled)\n",
    "\n",
    "# Train engineered features model (already done in previous cells, but ensuring same split)\n",
    "model_eng = LogisticRegression(class_weight='balanced', random_state=RANDOM_SEED, max_iter=1000)\n",
    "model_eng.fit(X_train_scaled, y_train)\n",
    "y_pred_eng = model_eng.predict(X_test_scaled)\n",
    "\n",
    "# Verify same test set (should be True)\n",
    "assert np.array_equal(y_orig_test, y_test), \"Test sets don't match! Check random seed.\"\n",
    "\n",
    "# Compare metrics on the SAME test set\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ORIGINAL vs ENGINEERED FEATURES COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "metrics = {\n",
    "    'Features Count': [len(original_features), len(feature_cols)],\n",
    "    'Precision': [precision_score(y_orig_test, y_pred_orig), precision_score(y_test, y_pred_eng)],\n",
    "    'Recall': [recall_score(y_orig_test, y_pred_orig), recall_score(y_test, y_pred_eng)],\n",
    "    'F1 Score': [f1_score(y_orig_test, y_pred_orig), f1_score(y_test, y_pred_eng)]\n",
    "}\n",
    "\n",
    "comparison = pd.DataFrame(metrics, index=['Original', 'Engineered'])\n",
    "print(comparison)\n",
    "\n",
    "improvement = ((comparison.loc['Engineered', 'F1 Score'] - comparison.loc['Original', 'F1 Score']) /\n",
    "               comparison.loc['Original', 'F1 Score'] * 100)\n",
    "print(f\"\\nF1 Score Improvement: {improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Top Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Most Important Features:\n",
      "============================================================\n",
      "valence_x_energy               +2.0903  [ENGINEERED]\n",
      "energy_squared                 -1.6221  [ENGINEERED]\n",
      "danceability                   +1.4115  [ORIGINAL]\n",
      "energy_x_danceability          -1.3599  [ENGINEERED]\n",
      "valence_squared                -1.2063  [ENGINEERED]\n",
      "loudness                       +1.0922  [ORIGINAL]\n",
      "instrumentalness               -1.0099  [ORIGINAL]\n",
      "valence                        -0.9958  [ORIGINAL]\n",
      "energy                         +0.9538  [ORIGINAL]\n",
      "acoustic_vs_energy             -0.5369  [ENGINEERED]\n",
      "\n",
      "5/10 top features are engineered\n"
     ]
    }
   ],
   "source": [
    "# Feature importance from coefficients\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Coefficient': model_eng.coef_[0],\n",
    "    'Abs_Coefficient': np.abs(model_eng.coef_[0])\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(\"=\"*60)\n",
    "for idx, row in feature_importance.head(10).iterrows():\n",
    "    feature_type = \"ENGINEERED\" if row['Feature'] not in original_features else \"ORIGINAL\"\n",
    "    print(f\"{row['Feature']:30s} {row['Coefficient']:+.4f}  [{feature_type}]\")\n",
    "\n",
    "# Count engineered features in top 10\n",
    "top10_engineered = sum(1 for f in feature_importance.head(10)['Feature'] if f not in original_features)\n",
    "print(f\"\\n{top10_engineered}/10 top features are engineered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Engineered Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved engineered dataset to: c:\\Users\\aruni\\FeatureBeats\\data\\processed\\hits_dataset_engineered.csv\n",
      "   Original features: 9\n",
      "   Total features: 19\n",
      "   New features: 10\n",
      "\n",
      "Dataset info:\n",
      "   Rows: 113,999\n",
      "   Columns: 23\n",
      "   File size: 24436.6 KB\n"
     ]
    }
   ],
   "source": [
    "# Ensure output directory exists\n",
    "processed_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save engineered dataset\n",
    "output_file = processed_data_dir / 'hits_dataset_engineered.csv'\n",
    "df_engineered.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Saved engineered dataset to: {output_file}\")\n",
    "print(f\"   Original features: {len(original_features)}\")\n",
    "print(f\"   Total features: {len(feature_cols)}\")\n",
    "print(f\"   New features: {len(feature_cols) - len(original_features)}\")\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"   Rows: {df_engineered.shape[0]:,}\")\n",
    "print(f\"   Columns: {df_engineered.shape[1]}\")\n",
    "print(f\"   File size: {output_file.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "### New Features Created:\n",
    "1. **Interaction Terms**: energy×danceability, valence×energy, etc.\n",
    "2. **Polynomial Features**: Squared terms for key features\n",
    "3. **Domain Features**: party_factor, acoustic_contrast\n",
    "4. **Temporal Features**: year_normalized, year_period\n",
    "\n",
    "### Impact:\n",
    "- Original features: Basic Spotify audio features\n",
    "- Engineered features: Enhanced with domain knowledge\n",
    "- Can improve model performance by capturing complex patterns\n",
    "\n",
    "### Usage:\n",
    "Use `hits_dataset_engineered.csv` in subsequent modeling for potentially better results!\n",
    "\n",
    "\n",
    "Feature engineering adds 10 new predictors and leads to a modest but meaningful performance boost: the F1 score improves by about 2.5%, and several engineered features rank among the most important in the model, confirming they add useful signal. Key contributors include valence x energy, energy^2 and energy x danceability. Overall, we learn that simple domain-informed transformations help the model capture more complex patterns, and the engineered dataset provides a stronger foundation for future models like XGBoost.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
