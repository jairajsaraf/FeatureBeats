{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Sampling Techniques: SMOTE & Variants\n",
    "\n",
    "## Objectives\n",
    "1. Apply SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "2. Compare different sampling strategies:\n",
    "   - SMOTE\n",
    "   - ADASYN (Adaptive Synthetic Sampling)\n",
    "   - BorderlineSMOTE\n",
    "   - SMOTE + Tomek Links\n",
    "3. Evaluate impact on model performance\n",
    "4. Compare with baseline class weighting approach\n",
    "\n",
    "## When to Use SMOTE\n",
    "- Severe class imbalance (>10:1 ratio)\n",
    "- Small minority class sample size\n",
    "- When class weighting alone isn't sufficient\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import joblib\n",
    "from collections import Counter\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score, average_precision_score\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "# Sampling techniques\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "project_root = Path.cwd().parent\n",
    "data_dir = project_root / 'data'\n",
    "processed_data_dir = data_dir / 'processed'\n",
    "models_dir = project_root / 'models'\n",
    "figures_dir = project_root / 'figures'\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_file = processed_data_dir / 'hits_dataset.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "\n",
    "# Prepare features and target\n",
    "exclude_cols = ['is_hit', 'year']\n",
    "text_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "exclude_cols.extend(text_cols)\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df['is_hit'].values\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  Class {label}: {count:,} ({count/len(y)*100:.2f}%)\")\n",
    "print(f\"\\nImbalance ratio: {counts[0]/counts[1]:.1f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Train set: {len(X_train):,} samples\")\n",
    "print(f\"Test set:  {len(X_test):,} samples\")\n",
    "print(f\"\\nTrain class distribution: {Counter(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline (No Sampling)\n",
    "\n",
    "First, let's establish a baseline using class weighting only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline model\n",
    "print(\"Training baseline (class weighting only)...\")\n",
    "\n",
    "baseline_model = LogisticRegression(\n",
    "    class_weight='balanced',\n",
    "    random_state=RANDOM_SEED,\n",
    "    max_iter=1000\n",
    ")\n",
    "baseline_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_baseline = baseline_model.predict(X_test_scaled)\n",
    "y_pred_proba_baseline = baseline_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "baseline_metrics = {\n",
    "    'Method': 'Baseline (Class Weighting)',\n",
    "    'Train Samples': len(X_train),\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_baseline),\n",
    "    'Precision': precision_score(y_test, y_pred_baseline),\n",
    "    'Recall': recall_score(y_test, y_pred_baseline),\n",
    "    'F1': f1_score(y_test, y_pred_baseline),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_pred_proba_baseline),\n",
    "    'PR-AUC': average_precision_score(y_test, y_pred_proba_baseline)\n",
    "}\n",
    "\n",
    "print(\"\\nBaseline Results:\")\n",
    "for metric, value in baseline_metrics.items():\n",
    "    if isinstance(value, (int, float)) and metric != 'Train Samples':\n",
    "        print(f\"  {metric:12s}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {metric:12s}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SMOTE (Synthetic Minority Over-sampling)\n",
    "\n",
    "SMOTE creates synthetic samples by:\n",
    "1. Finding k-nearest neighbors of minority class samples\n",
    "2. Interpolating between samples and their neighbors\n",
    "3. Creating new synthetic minority samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE\n",
    "print(\"Applying SMOTE...\")\n",
    "\n",
    "smote = SMOTE(random_state=RANDOM_SEED, k_neighbors=5)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nOriginal training set: {Counter(y_train)}\")\n",
    "print(f\"After SMOTE:           {Counter(y_train_smote)}\")\n",
    "print(f\"\\nNew training set size: {len(X_train_smote):,} (increased by {len(X_train_smote) - len(X_train):,})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on SMOTE data\n",
    "print(\"Training model with SMOTE...\")\n",
    "\n",
    "smote_model = LogisticRegression(random_state=RANDOM_SEED, max_iter=1000)\n",
    "smote_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_smote = smote_model.predict(X_test_scaled)\n",
    "y_pred_proba_smote = smote_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "smote_metrics = {\n",
    "    'Method': 'SMOTE',\n",
    "    'Train Samples': len(X_train_smote),\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_smote),\n",
    "    'Precision': precision_score(y_test, y_pred_smote),\n",
    "    'Recall': recall_score(y_test, y_pred_smote),\n",
    "    'F1': f1_score(y_test, y_pred_smote),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_pred_proba_smote),\n",
    "    'PR-AUC': average_precision_score(y_test, y_pred_proba_smote)\n",
    "}\n",
    "\n",
    "print(\"\\nSMOTE Results:\")\n",
    "for metric, value in smote_metrics.items():\n",
    "    if isinstance(value, (int, float)) and metric != 'Train Samples':\n",
    "        print(f\"  {metric:12s}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {metric:12s}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ADASYN (Adaptive Synthetic Sampling)\n",
    "\n",
    "ADASYN is similar to SMOTE but generates more synthetic samples for minority class examples that are harder to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ADASYN\n",
    "print(\"Applying ADASYN...\")\n",
    "\n",
    "adasyn = ADASYN(random_state=RANDOM_SEED, n_neighbors=5)\n",
    "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nOriginal training set: {Counter(y_train)}\")\n",
    "print(f\"After ADASYN:          {Counter(y_train_adasyn)}\")\n",
    "\n",
    "# Train model\n",
    "adasyn_model = LogisticRegression(random_state=RANDOM_SEED, max_iter=1000)\n",
    "adasyn_model.fit(X_train_adasyn, y_train_adasyn)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_adasyn = adasyn_model.predict(X_test_scaled)\n",
    "y_pred_proba_adasyn = adasyn_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "adasyn_metrics = {\n",
    "    'Method': 'ADASYN',\n",
    "    'Train Samples': len(X_train_adasyn),\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_adasyn),\n",
    "    'Precision': precision_score(y_test, y_pred_adasyn),\n",
    "    'Recall': recall_score(y_test, y_pred_adasyn),\n",
    "    'F1': f1_score(y_test, y_pred_adasyn),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_pred_proba_adasyn),\n",
    "    'PR-AUC': average_precision_score(y_test, y_pred_proba_adasyn)\n",
    "}\n",
    "\n",
    "print(\"\\nADASYN Results:\")\n",
    "for metric, value in adasyn_metrics.items():\n",
    "    if isinstance(value, (int, float)) and metric != 'Train Samples':\n",
    "        print(f\"  {metric:12s}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {metric:12s}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Borderline-SMOTE\n",
    "\n",
    "Focuses on borderline samples (minority samples near the decision boundary) which are often more important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Borderline-SMOTE\n",
    "print(\"Applying Borderline-SMOTE...\")\n",
    "\n",
    "borderline_smote = BorderlineSMOTE(random_state=RANDOM_SEED, k_neighbors=5)\n",
    "X_train_bsmote, y_train_bsmote = borderline_smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nOriginal training set:    {Counter(y_train)}\")\n",
    "print(f\"After Borderline-SMOTE:   {Counter(y_train_bsmote)}\")\n",
    "\n",
    "# Train model\n",
    "bsmote_model = LogisticRegression(random_state=RANDOM_SEED, max_iter=1000)\n",
    "bsmote_model.fit(X_train_bsmote, y_train_bsmote)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_bsmote = bsmote_model.predict(X_test_scaled)\n",
    "y_pred_proba_bsmote = bsmote_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "bsmote_metrics = {\n",
    "    'Method': 'Borderline-SMOTE',\n",
    "    'Train Samples': len(X_train_bsmote),\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_bsmote),\n",
    "    'Precision': precision_score(y_test, y_pred_bsmote),\n",
    "    'Recall': recall_score(y_test, y_pred_bsmote),\n",
    "    'F1': f1_score(y_test, y_pred_bsmote),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_pred_proba_bsmote),\n",
    "    'PR-AUC': average_precision_score(y_test, y_pred_proba_bsmote)\n",
    "}\n",
    "\n",
    "print(\"\\nBorderline-SMOTE Results:\")\n",
    "for metric, value in bsmote_metrics.items():\n",
    "    if isinstance(value, (int, float)) and metric != 'Train Samples':\n",
    "        print(f\"  {metric:12s}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {metric:12s}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SMOTE + Tomek Links\n",
    "\n",
    "Combines oversampling (SMOTE) with cleaning (removes Tomek links - pairs of opposite-class samples that are each other's nearest neighbors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE + Tomek\n",
    "print(\"Applying SMOTE + Tomek Links...\")\n",
    "\n",
    "smote_tomek = SMOTETomek(random_state=RANDOM_SEED)\n",
    "X_train_st, y_train_st = smote_tomek.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nOriginal training set: {Counter(y_train)}\")\n",
    "print(f\"After SMOTE+Tomek:     {Counter(y_train_st)}\")\n",
    "\n",
    "# Train model\n",
    "st_model = LogisticRegression(random_state=RANDOM_SEED, max_iter=1000)\n",
    "st_model.fit(X_train_st, y_train_st)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_st = st_model.predict(X_test_scaled)\n",
    "y_pred_proba_st = st_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "st_metrics = {\n",
    "    'Method': 'SMOTE+Tomek',\n",
    "    'Train Samples': len(X_train_st),\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_st),\n",
    "    'Precision': precision_score(y_test, y_pred_st),\n",
    "    'Recall': recall_score(y_test, y_pred_st),\n",
    "    'F1': f1_score(y_test, y_pred_st),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_pred_proba_st),\n",
    "    'PR-AUC': average_precision_score(y_test, y_pred_proba_st)\n",
    "}\n",
    "\n",
    "print(\"\\nSMOTE+Tomek Results:\")\n",
    "for metric, value in st_metrics.items():\n",
    "    if isinstance(value, (int, float)) and metric != 'Train Samples':\n",
    "        print(f\"  {metric:12s}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {metric:12s}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison of All Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame([\n",
    "    baseline_metrics,\n",
    "    smote_metrics,\n",
    "    adasyn_metrics,\n",
    "    bsmote_metrics,\n",
    "    st_metrics\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"SAMPLING METHODS COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Bar chart comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC-AUC', 'PR-AUC']\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(comparison_df)))\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    values = comparison_df[metric].values\n",
    "    methods = comparison_df['Method'].values\n",
    "    \n",
    "    bars = ax.bar(range(len(methods)), values, color=colors)\n",
    "    ax.set_xticks(range(len(methods)))\n",
    "    ax.set_xticklabels(methods, rotation=45, ha='right', fontsize=9)\n",
    "    ax.set_ylabel(metric, fontsize=11)\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_ylim([0, 1])\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # Highlight best performer\n",
    "    best_idx = values.argmax()\n",
    "    bars[best_idx].set_edgecolor('red')\n",
    "    bars[best_idx].set_linewidth(3)\n",
    "\n",
    "plt.suptitle('Sampling Methods Comparison - All Metrics', \n",
    "             fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'sampling_methods_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Saved: sampling_methods_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Method Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best method for each metric\n",
    "print(\"BEST METHODS BY METRIC\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for metric in metrics_to_plot:\n",
    "    best_idx = comparison_df[metric].idxmax()\n",
    "    best_method = comparison_df.loc[best_idx, 'Method']\n",
    "    best_value = comparison_df.loc[best_idx, metric]\n",
    "    print(f\"{metric:12s}: {best_method:20s} ({best_value:.4f})\")\n",
    "\n",
    "# Overall best (based on F1 score)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "best_f1_idx = comparison_df['F1'].idxmax()\n",
    "best_method = comparison_df.loc[best_f1_idx, 'Method']\n",
    "print(f\"\\nðŸ† OVERALL BEST METHOD (by F1 Score): {best_method}\")\n",
    "print(\"\\nPerformance:\")\n",
    "for metric in metrics_to_plot:\n",
    "    value = comparison_df.loc[best_f1_idx, metric]\n",
    "    print(f\"  {metric:12s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Confusion Matrices Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for all methods\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "predictions = [\n",
    "    ('Baseline', y_pred_baseline),\n",
    "    ('SMOTE', y_pred_smote),\n",
    "    ('ADASYN', y_pred_adasyn),\n",
    "    ('Borderline-SMOTE', y_pred_bsmote),\n",
    "    ('SMOTE+Tomek', y_pred_st)\n",
    "]\n",
    "\n",
    "for idx, (method, y_pred) in enumerate(predictions):\n",
    "    ax = axes[idx]\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Non-Hit', 'Hit'],\n",
    "                yticklabels=['Non-Hit', 'Hit'],\n",
    "                cbar=False, ax=ax)\n",
    "    \n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_title(f'{method}', fontweight='bold')\n",
    "\n",
    "# Hide last subplot\n",
    "axes[-1].set_visible(False)\n",
    "\n",
    "plt.suptitle('Confusion Matrices - All Sampling Methods', \n",
    "             fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'sampling_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Saved: sampling_confusion_matrices.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compare best sampling vs baseline\n",
    "baseline_f1 = baseline_metrics['F1']\n",
    "best_f1 = comparison_df['F1'].max()\n",
    "improvement = ((best_f1 - baseline_f1) / baseline_f1) * 100\n",
    "\n",
    "print(f\"\\n1. PERFORMANCE IMPROVEMENT\")\n",
    "print(f\"   Baseline F1:     {baseline_f1:.4f}\")\n",
    "print(f\"   Best Method F1:  {best_f1:.4f}\")\n",
    "print(f\"   Improvement:     {improvement:+.2f}%\")\n",
    "\n",
    "if improvement > 5:\n",
    "    print(f\"\\n   âœ… SIGNIFICANT IMPROVEMENT - Use sampling methods!\")\n",
    "elif improvement > 0:\n",
    "    print(f\"\\n   âœ… MODEST IMPROVEMENT - Sampling helps but consider trade-offs\")\n",
    "else:\n",
    "    print(f\"\\n   âš ï¸  NO IMPROVEMENT - Stick with class weighting\")\n",
    "\n",
    "print(f\"\\n2. METHOD SELECTION GUIDE\")\n",
    "print(f\"\\n   Use SMOTE when:\")\n",
    "print(f\"   â€¢ You have severe imbalance (>20:1)\")\n",
    "print(f\"   â€¢ Small minority class size (<500 samples)\")\n",
    "print(f\"   â€¢ Data is well-distributed\")\n",
    "\n",
    "print(f\"\\n   Use ADASYN when:\")\n",
    "print(f\"   â€¢ Data has varying density\")\n",
    "print(f\"   â€¢ Some minority samples are harder to learn\")\n",
    "print(f\"   â€¢ You want adaptive oversampling\")\n",
    "\n",
    "print(f\"\\n   Use Borderline-SMOTE when:\")\n",
    "print(f\"   â€¢ Decision boundary is complex\")\n",
    "print(f\"   â€¢ You want to focus on borderline cases\")\n",
    "print(f\"   â€¢ More conservative than standard SMOTE\")\n",
    "\n",
    "print(f\"\\n   Use SMOTE+Tomek when:\")\n",
    "print(f\"   â€¢ Data has noise/outliers\")\n",
    "print(f\"   â€¢ Classes overlap significantly\")\n",
    "print(f\"   â€¢ You want cleaner decision boundaries\")\n",
    "\n",
    "print(f\"\\n3. TRADE-OFFS\")\n",
    "print(f\"\\n   Pros of Sampling:\")\n",
    "print(f\"   âœ… Often improves recall (catches more hits)\")\n",
    "print(f\"   âœ… Balanced training set\")\n",
    "print(f\"   âœ… Can help with complex decision boundaries\")\n",
    "\n",
    "print(f\"\\n   Cons of Sampling:\")\n",
    "print(f\"   âŒ Increased training time (more samples)\")\n",
    "print(f\"   âŒ Risk of overfitting to synthetic data\")\n",
    "print(f\"   âŒ May reduce precision\")\n",
    "print(f\"   âŒ Doesn't work well with very high dimensions\")\n",
    "\n",
    "print(f\"\\n4. FINAL RECOMMENDATION\")\n",
    "if improvement > 5:\n",
    "    print(f\"   ðŸŽ¯ Use {best_method} for production model\")\n",
    "    print(f\"   ðŸ“Š Expected F1 improvement: {improvement:.1f}%\")\n",
    "else:\n",
    "    print(f\"   ðŸŽ¯ Stick with baseline class weighting approach\")\n",
    "    print(f\"   ðŸ“Š Sampling doesn't provide significant benefit\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best performing model and data\n",
    "best_idx = comparison_df['F1'].idxmax()\n",
    "best_method_name = comparison_df.loc[best_idx, 'Method']\n",
    "\n",
    "# Map to actual model\n",
    "model_map = {\n",
    "    'Baseline (Class Weighting)': baseline_model,\n",
    "    'SMOTE': smote_model,\n",
    "    'ADASYN': adasyn_model,\n",
    "    'Borderline-SMOTE': bsmote_model,\n",
    "    'SMOTE+Tomek': st_model\n",
    "}\n",
    "\n",
    "best_model = model_map[best_method_name]\n",
    "\n",
    "# Save model\n",
    "joblib.dump(best_model, models_dir / 'best_sampling_model.pkl')\n",
    "print(f\"âœ… Saved best model ({best_method_name}) to: {models_dir / 'best_sampling_model.pkl'}\")\n",
    "\n",
    "# Save comparison metrics\n",
    "comparison_df.to_csv(models_dir / 'sampling_methods_comparison.csv', index=False)\n",
    "print(f\"âœ… Saved comparison metrics to: {models_dir / 'sampling_methods_comparison.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Advanced Sampling Complete!\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **SMOTE and variants** can help with severe class imbalance\n",
    "2. **Different methods** excel at different aspects:\n",
    "   - SMOTE: General purpose, balanced oversampling\n",
    "   - ADASYN: Adaptive, focuses on hard-to-learn samples\n",
    "   - Borderline-SMOTE: Conservative, focuses on decision boundary\n",
    "   - SMOTE+Tomek: Includes cleaning step for noisy data\n",
    "\n",
    "3. **Trade-offs exist**:\n",
    "   - Improved recall often comes with reduced precision\n",
    "   - More training samples = longer training time\n",
    "   - Risk of overfitting to synthetic data\n",
    "\n",
    "4. **Not always necessary**:\n",
    "   - If class weighting works well, stick with it\n",
    "   - Sampling is most helpful with severe imbalance\n",
    "\n",
    "### Files Created:\n",
    "- `figures/sampling_methods_comparison.png`\n",
    "- `figures/sampling_confusion_matrices.png`\n",
    "- `models/best_sampling_model.pkl`\n",
    "- `models/sampling_methods_comparison.csv`\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
