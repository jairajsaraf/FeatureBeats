{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba37425c",
   "metadata": {},
   "source": [
    "In this notebook, we directly compare how well models perform when trained on the original Spotify audio features versus the engineered feature set created earlier. We load both datasets, isolate numeric predictors, and use the same stratified train–test split to ensure a fair comparison. After scaling the features, we train two types of models on each feature set: a class-weighted logistic regression and an XGBoost classifier configured for heavy class imbalance. We then compute accuracy, precision, recall, F1, ROC-AUC, and PR-AUC for all four model–feature-set combinations, and save both the trained models and a metrics comparison table. This gives us a clean, controlled way to measure whether engineered features improve predictive performance across different model families."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6ebfbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING MODELS ON ENGINEERED FEATURES DATASET\n",
      "================================================================================\n",
      "\n",
      "1. Loading datasets...\n",
      "   Original: 113,999 rows 13 columns\n",
      "   Engineered: 113,999 rows 23 columns\n",
      "   New features added: 10\n",
      "\n",
      "2. Preparing original features...\n",
      "   Features: ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']\n",
      "\n",
      "3. Preparing engineered features...\n",
      "   Features: ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'energy_x_danceability', 'valence_x_energy', 'loudness_x_energy', 'acoustic_vs_energy', 'party_factor', 'danceability_squared', 'energy_squared', 'valence_squared', 'year_normalized', 'year_period']\n",
      "\n",
      "4. Creating train/test splits...\n",
      "   Train: 91,199 samples (1710 hits)\n",
      "   Test:  22,800 samples (428 hits)\n",
      "\n",
      "5. Scaling features...\n",
      "\n",
      "6. Training Logistic Regression models...\n",
      "   Original features...\n",
      "   Engineered features...\n",
      "\n",
      "7. Training XGBoost models...\n",
      "   Original features...\n",
      "   Engineered features...\n",
      "\n",
      "8. Calculating metrics...\n",
      "\n",
      "9. Saving models...\n",
      "Saved engineered models\n",
      "\n",
      "10. Saving metrics...\n",
      "Saved comparison metrics\n",
      "\n",
      "================================================================================\n",
      "RESULTS: ORIGINAL vs ENGINEERED FEATURES\n",
      "================================================================================\n",
      "\n",
      "             Model  Features  Accuracy  Precision   Recall       F1  ROC-AUC   PR-AUC\n",
      "   LogReg-Original         9  0.619781   0.039352 0.822430 0.075109 0.768790 0.054631\n",
      " LogReg-Engineered        19  0.628684   0.040371 0.824766 0.076973 0.773610 0.058474\n",
      "  XGBoost-Original         9  0.794781   0.073621 0.857477 0.135599 0.910904 0.321283\n",
      "XGBoost-Engineered        19  0.797982   0.075234 0.864486 0.138421 0.916238 0.356513\n",
      "\n",
      "================================================================================\n",
      "IMPROVEMENT ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "LogReg:\n",
      "   Original F1:    0.0751\n",
      "   Engineered F1:  0.0770\n",
      "   Improvement:    +2.48%\n",
      "\n",
      "XGBoost:\n",
      "   Original F1:    0.1356\n",
      "   Engineered F1:  0.1384\n",
      "   Improvement:    +2.08%\n",
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Train models on engineered features dataset and compare with original features\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, confusion_matrix\n",
    ")\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING MODELS ON ENGINEERED FEATURES DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Setup paths\n",
    "project_root = Path.cwd().parent\n",
    "data_dir = project_root / 'data' / 'processed'\n",
    "models_dir = project_root / 'models'\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Load both datasets\n",
    "print(\"\\n1. Loading datasets...\")\n",
    "df_original = pd.read_csv(data_dir / 'hits_dataset.csv')\n",
    "df_engineered = pd.read_csv(data_dir / 'hits_dataset_engineered.csv')\n",
    "\n",
    "print(f\"   Original: {df_original.shape[0]:,} rows {df_original.shape[1]} columns\")\n",
    "print(f\"   Engineered: {df_engineered.shape[0]:,} rows {df_engineered.shape[1]} columns\")\n",
    "print(f\"   New features added: {df_engineered.shape[1] - df_original.shape[1]}\")\n",
    "\n",
    "# Prepare original features\n",
    "print(\"\\n2. Preparing original features...\")\n",
    "exclude_cols = ['is_hit', 'year', 'track_name', 'artists']\n",
    "original_features = [col for col in df_original.columns\n",
    "                     if col not in exclude_cols and df_original[col].dtype in ['float64', 'int64']]\n",
    "print(f\"   Features: {original_features}\")\n",
    "\n",
    "X_orig = df_original[original_features].values\n",
    "y = df_original['is_hit'].values\n",
    "\n",
    "# Prepare engineered features\n",
    "print(\"\\n3. Preparing engineered features...\")\n",
    "engineered_features = [col for col in df_engineered.columns\n",
    "                       if col not in exclude_cols and df_engineered[col].dtype in ['float64', 'int64']]\n",
    "print(f\"   Features: {engineered_features}\")\n",
    "\n",
    "X_eng = df_engineered[engineered_features].values\n",
    "\n",
    "# Create same train/test split for fair comparison\n",
    "print(\"\\n4. Creating train/test splits...\")\n",
    "indices = np.arange(len(y))\n",
    "train_idx, test_idx = train_test_split(\n",
    "    indices, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_orig_train = X_orig[train_idx]\n",
    "X_orig_test = X_orig[test_idx]\n",
    "X_eng_train = X_eng[train_idx]\n",
    "X_eng_test = X_eng[test_idx]\n",
    "y_train = y[train_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "print(f\"   Train: {len(train_idx):,} samples ({y_train.sum()} hits)\")\n",
    "print(f\"   Test:  {len(test_idx):,} samples ({y_test.sum()} hits)\")\n",
    "\n",
    "# Scale features\n",
    "print(\"\\n5. Scaling features...\")\n",
    "scaler_orig = StandardScaler()\n",
    "X_orig_train_scaled = scaler_orig.fit_transform(X_orig_train)\n",
    "X_orig_test_scaled = scaler_orig.transform(X_orig_test)\n",
    "\n",
    "scaler_eng = StandardScaler()\n",
    "X_eng_train_scaled = scaler_eng.fit_transform(X_eng_train)\n",
    "X_eng_test_scaled = scaler_eng.transform(X_eng_test)\n",
    "\n",
    "# Train Logistic Regression models\n",
    "print(\"\\n6. Training Logistic Regression models...\")\n",
    "print(\"   Original features...\")\n",
    "lr_orig = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)\n",
    "lr_orig.fit(X_orig_train_scaled, y_train)\n",
    "y_pred_lr_orig = lr_orig.predict(X_orig_test_scaled)\n",
    "y_proba_lr_orig = lr_orig.predict_proba(X_orig_test_scaled)[:, 1]\n",
    "\n",
    "print(\"   Engineered features...\")\n",
    "lr_eng = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)\n",
    "lr_eng.fit(X_eng_train_scaled, y_train)\n",
    "y_pred_lr_eng = lr_eng.predict(X_eng_test_scaled)\n",
    "y_proba_lr_eng = lr_eng.predict_proba(X_eng_test_scaled)[:, 1]\n",
    "\n",
    "# Train XGBoost models\n",
    "print(\"\\n7. Training XGBoost models...\")\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "print(\"   Original features...\")\n",
    "xgb_orig = xgb.XGBClassifier(\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_orig.fit(X_orig_train_scaled, y_train, verbose=False)\n",
    "y_pred_xgb_orig = xgb_orig.predict(X_orig_test_scaled)\n",
    "y_proba_xgb_orig = xgb_orig.predict_proba(X_orig_test_scaled)[:, 1]\n",
    "\n",
    "print(\"   Engineered features...\")\n",
    "xgb_eng = xgb.XGBClassifier(\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_eng.fit(X_eng_train_scaled, y_train, verbose=False)\n",
    "y_pred_xgb_eng = xgb_eng.predict(X_eng_test_scaled)\n",
    "y_proba_xgb_eng = xgb_eng.predict_proba(X_eng_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"\\n8. Calculating metrics...\")\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_proba):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred),\n",
    "        'f1_score': f1_score(y_true, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_true, y_proba),\n",
    "        'pr_auc': average_precision_score(y_true, y_proba)\n",
    "    }\n",
    "\n",
    "metrics_lr_orig = calculate_metrics(y_test, y_pred_lr_orig, y_proba_lr_orig)\n",
    "metrics_lr_eng = calculate_metrics(y_test, y_pred_lr_eng, y_proba_lr_eng)\n",
    "metrics_xgb_orig = calculate_metrics(y_test, y_pred_xgb_orig, y_proba_xgb_orig)\n",
    "metrics_xgb_eng = calculate_metrics(y_test, y_pred_xgb_eng, y_proba_xgb_eng)\n",
    "\n",
    "# Save models\n",
    "print(\"\\n9. Saving models...\")\n",
    "joblib.dump(lr_eng, models_dir / 'logreg_engineered.pkl')\n",
    "joblib.dump(xgb_eng, models_dir / 'xgboost_engineered.pkl')\n",
    "joblib.dump(scaler_eng, models_dir / 'scaler_engineered.pkl')\n",
    "print(\"Saved engineered models\")\n",
    "\n",
    "# Save metrics\n",
    "print(\"\\n10. Saving metrics...\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['LogReg-Original', 'LogReg-Engineered', 'XGBoost-Original', 'XGBoost-Engineered'],\n",
    "    'Features': [len(original_features), len(engineered_features),\n",
    "                 len(original_features), len(engineered_features)],\n",
    "    'Accuracy': [metrics_lr_orig['accuracy'], metrics_lr_eng['accuracy'],\n",
    "                 metrics_xgb_orig['accuracy'], metrics_xgb_eng['accuracy']],\n",
    "    'Precision': [metrics_lr_orig['precision'], metrics_lr_eng['precision'],\n",
    "                  metrics_xgb_orig['precision'], metrics_xgb_eng['precision']],\n",
    "    'Recall': [metrics_lr_orig['recall'], metrics_lr_eng['recall'],\n",
    "               metrics_xgb_orig['recall'], metrics_xgb_eng['recall']],\n",
    "    'F1': [metrics_lr_orig['f1_score'], metrics_lr_eng['f1_score'],\n",
    "           metrics_xgb_orig['f1_score'], metrics_xgb_eng['f1_score']],\n",
    "    'ROC-AUC': [metrics_lr_orig['roc_auc'], metrics_lr_eng['roc_auc'],\n",
    "                metrics_xgb_orig['roc_auc'], metrics_xgb_eng['roc_auc']],\n",
    "    'PR-AUC': [metrics_lr_orig['pr_auc'], metrics_lr_eng['pr_auc'],\n",
    "               metrics_xgb_orig['pr_auc'], metrics_xgb_eng['pr_auc']]\n",
    "})\n",
    "\n",
    "comparison_df.to_csv(models_dir / 'engineered_features_comparison.csv', index=False)\n",
    "print(\"Saved comparison metrics\")\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS: ORIGINAL vs ENGINEERED FEATURES\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "# Calculate improvements\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPROVEMENT ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_type in ['LogReg', 'XGBoost']:\n",
    "    orig_f1 = comparison_df[comparison_df['Model'] == f'{model_type}-Original']['F1'].values[0]\n",
    "    eng_f1 = comparison_df[comparison_df['Model'] == f'{model_type}-Engineered']['F1'].values[0]\n",
    "    improvement = ((eng_f1 - orig_f1) / orig_f1 * 100) if orig_f1 > 0 else 0\n",
    "\n",
    "    print(f\"\\n{model_type}:\")\n",
    "    print(f\"   Original F1:    {orig_f1:.4f}\")\n",
    "    print(f\"   Engineered F1:  {eng_f1:.4f}\")\n",
    "    print(f\"   Improvement:    {improvement:+.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee4fc2f",
   "metadata": {},
   "source": [
    "The engineered features provide a small but consistent performance boost across both models. Logistic regression’s F1 improves by about 2.5%, and XGBoost shows a similar 2% gain, with engineered features also slightly improving precision, recall, and both AUC metrics. While the improvements are modest, they are steady across models, confirming that interaction terms, polynomial features, and domain-driven transformations help the models capture patterns not visible in the raw audio features. Overall, this shows that engineered features meaningfully strengthen the feature space and offer measurable benefits before moving on to more advanced modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
